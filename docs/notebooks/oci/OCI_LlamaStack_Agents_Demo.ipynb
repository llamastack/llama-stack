{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df693aa",
   "metadata": {},
   "source": [
    "# Building AI Agents with OCI and Llama Stack\n",
    "\n",
    "This notebook demonstrates how to build Agents using Oracle Cloud Infrastructure (OCI) Generative AI models through Llama Stack.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Basic Agents** - Create agents with custom instructions\n",
    "2. **Multi-Turn Conversations** - Agents that maintain context across interactions\n",
    "3. **Agentic Workflows** - Chain multiple agent capabilities together\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Install required packages:**\n",
    "   ```bash\n",
    "   pip install llama-stack-client oci\n",
    "   ```\n",
    "\n",
    "2. **Configure OCI credentials:**\n",
    "   - Set up `~/.oci/config` with your OCI credentials\n",
    "   - Set the `OCI_COMPARTMENT_OCID` environment variable\n",
    "   - Set the `OCI_REGION` environment variable\n",
    "   - Set the `OCI_AUTH_TYPE` environment variable with 'config_file'\n",
    "\n",
    "3. **Start Llama Stack server with OCI provider:**\n",
    "   ```bash\n",
    "   llama stack run /path/to/your_oci_config.yaml\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84fa9c7",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb7d27b-4259-4f9e-aef1-96857bbfaa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path updated to use venv\n"
     ]
    }
   ],
   "source": [
    "# Make sure that your llama stack client is compatible with your llama stack server\n",
    "# Optional in case you need to select a specific venv enviornment.\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/omara/oci/venv/lib/python3.12/site-packages')\n",
    "print(f\"Python path updated to use venv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d385d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OCI_COMPARTMENT_OCID is set\n",
      "âœ… Connected to Llama Stack server\n",
      "\n",
      "ðŸ“‹ Available models:\n",
      "   â€¢ oci/google.gemini-2.5-flash\n",
      "   â€¢ oci/google.gemini-2.5-pro\n",
      "   â€¢ oci/google.gemini-2.5-flash-lite\n",
      "   â€¢ oci/xai.grok-4-fast-non-reasoning\n",
      "   â€¢ oci/xai.grok-4-fast-reasoning\n",
      "   â€¢ oci/xai.grok-code-fast-1\n",
      "   â€¢ oci/xai.grok-4\n",
      "   â€¢ oci/xai.grok-3-mini-fast\n",
      "   â€¢ oci/xai.grok-3-fast\n",
      "   â€¢ oci/xai.grok-3\n",
      "   â€¢ oci/xai.grok-3-mini\n",
      "\n",
      " Using model: oci/google.gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from llama_stack_client import LlamaStackClient, Agent\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Check environment\n",
    "if not os.getenv(\"OCI_COMPARTMENT_OCID\"):\n",
    "    print(\"âš ï¸  WARNING: OCI_COMPARTMENT_OCID environment variable not set\")\n",
    "    print(\"Please set it with: export OCI_COMPARTMENT_OCID='ocid1.compartment.oc1..xxx'\")\n",
    "else:\n",
    "    print(\"âœ… OCI_COMPARTMENT_OCID is set\")\n",
    "\n",
    "# Initialize client\n",
    "client = LlamaStackClient(base_url=\"http://localhost:8321\")\n",
    "print(\"âœ… Connected to Llama Stack server\")\n",
    "\n",
    "# Get available models\n",
    "models = client.models.list()\n",
    "print(f\"\\nðŸ“‹ Available models:\")\n",
    "for model in models:\n",
    "    print(f\"   â€¢ {model.id}\")\n",
    "\n",
    "# Select primary model\n",
    "model_id = models[0].id\n",
    "print(f\"\\n Using model: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15cb98",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Creating a Basic Agent\n",
    "\n",
    "Agents in Llama Stack are autonomous AI systems with:\n",
    "- **Instructions**: System-level guidance that shapes behavior\n",
    "- **Sessions**: Conversation contexts that maintain history\n",
    "- **Turns**: Individual interactions within a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c898f8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created OCI Expert Agent\n"
     ]
    }
   ],
   "source": [
    "# Create a specialized agent with custom instructions\n",
    "cloud_expert_agent = Agent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are an Oracle Cloud Infrastructure (OCI) expert assistant.\n",
    "You help users understand and work with OCI services including:\n",
    "- Compute instances and shapes\n",
    "- Networking (VCN, subnets, security lists)\n",
    "- Storage (Block, Object, File)\n",
    "- AI/ML services and GPU instances\n",
    "- Identity and Access Management (IAM)\n",
    "\n",
    "Always provide practical, actionable advice with examples when possible.\n",
    "If you're unsure about something, say so clearly.\"\"\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Created OCI Expert Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b41a537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created session: conv_00838b9921668e9fc1e24431c2e749732d041b7a4e0af149\n"
     ]
    }
   ],
   "source": [
    "# Create a session for our agent\n",
    "session_id = cloud_expert_agent.create_session(session_name=\"oci_consultation\")\n",
    "print(f\"âœ… Created session: {session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b805e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for chatting with agents and models\n",
    "\n",
    "def chat_with_agent(agent, session_id, message, stream=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Send a message to an agent and get the full response.\n",
    "    Properly handles both text responses and tool calls.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ‘¤ User: {message}\\n\")\n",
    "    print(\"ðŸ¤– Agent: \", end='')\n",
    "\n",
    "    response = agent.create_turn(\n",
    "        session_id=session_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        stream=stream,\n",
    "    )\n",
    "\n",
    "    streamed_text = \"\"\n",
    "    tool_calls_made = []\n",
    "\n",
    "    for chunk in response:\n",
    "        event = chunk.event\n",
    "        event_type = event.event_type\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[DEBUG] Event type: {event_type}\")\n",
    "            print(f\"[DEBUG] Event: {event}\")\n",
    "\n",
    "        if event_type == \"step_started\":\n",
    "            # Check if this is a tool execution step\n",
    "            if hasattr(event, 'step_type'):\n",
    "                if event.step_type == \"tool_execution\":\n",
    "                    print(\"\\n    Executing tool...\", end='')\n",
    "\n",
    "        elif event_type == \"step_progress\":\n",
    "            # Extract text from step_progress events\n",
    "            if hasattr(event, 'delta'):\n",
    "                if hasattr(event.delta, 'text') and event.delta.text:\n",
    "                    text = event.delta.text\n",
    "                    streamed_text += text\n",
    "                    print(text, end='', flush=True)\n",
    "\n",
    "        elif event_type == \"step_completed\":\n",
    "            # Check for tool call results\n",
    "            if hasattr(event, 'step_details'):\n",
    "                step = event.step_details\n",
    "                if hasattr(step, 'tool_calls') and step.tool_calls:\n",
    "                    for tc in step.tool_calls:\n",
    "                        tool_name = tc.tool_name if hasattr(tc, 'tool_name') else 'unknown'\n",
    "                        tool_calls_made.append(tool_name)\n",
    "                        print(f\"\\n   âœ… Tool '{tool_name}' completed\")\n",
    "\n",
    "        elif event_type == \"turn_completed\":\n",
    "            # Turn is complete - get final text if available\n",
    "            if hasattr(event, 'turn') and hasattr(event.turn, 'output_message'):\n",
    "                output = event.turn.output_message\n",
    "                if hasattr(output, 'content') and output.content:\n",
    "                    # Get text from content (may be a list of content blocks)\n",
    "                    if isinstance(output.content, list):\n",
    "                        for block in output.content:\n",
    "                            if hasattr(block, 'text') and block.text:\n",
    "                                if not streamed_text:  # Only if we haven't streamed it already\n",
    "                                    print(block.text)\n",
    "                                    streamed_text = block.text\n",
    "                    elif hasattr(output.content, 'text'):\n",
    "                        if not streamed_text:\n",
    "                            print(output.content.text)\n",
    "                            streamed_text = output.content.text\n",
    "            break\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if tool_calls_made:\n",
    "        print(f\" Tools used: {', '.join(tool_calls_made)}\")\n",
    "\n",
    "    return streamed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbdb1ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¤ User: What GPU shapes are available on OCI for AI workloads?\n",
      "\n",
      "ðŸ¤– Agent: Oracle Cloud Infrastructure (OCI) offers a robust selection of GPU shapes, primarily from **NVIDIA**, designed to accelerate various AI, Machine Learning, Deep Learning, and High-Performance Computing (HPC) workloads. These instances provide massive parallelism and high memory bandwidth crucial for these demanding tasks.\n",
      "\n",
      "OCI categorizes its GPU offerings into two main types:\n",
      "\n",
      "1.  **Virtual Machine (VM) GPU Shapes:** These provide flexibility, easier scaling, and are great for development, smaller to medium-scale training, and inference workloads.\n",
      "2.  **Bare Metal (BM) GPU Shapes:** These offer direct access to the underlying hardware, maximizing performance by eliminating virtualization overhead. They are ideal for the largest, most demanding training jobs, distributed deep learning, and HPC.\n",
      "\n",
      "Here's a breakdown of the available GPU shapes for AI workloads on OCI:\n",
      "\n",
      "---\n",
      "\n",
      "### I. Virtual Machine (VM) GPU Shapes\n",
      "\n",
      "VM GPU shapes are denoted by `VM.GPU.` followed by the GPU type and count.\n",
      "\n",
      "1.  **NVIDIA A100 (Ampere Architecture)**\n",
      "    *   **Shapes:**\n",
      "        *   `VM.GPU.A100.1` (1x NVIDIA A100 GPU)\n",
      "        *   `VM.GPU.A100.2` (2x NVIDIA A100 GPUs)\n",
      "        *   `VM.GPU.A100.4` (4x NVIDIA A100 GPUs)\n",
      "    *   **Key Features:** Each A100 typically comes with 40GB or 80GB of HBM2 memory. These GPUs are powerful for advanced AI/ML training, featuring third-generation Tensor Cores and NVLink for high-speed inter-GPU communication within the instance.\n",
      "    *   **Use Cases:** Large-scale deep learning model training, scientific simulations, complex data analytics.\n",
      "\n",
      "2.  **NVIDIA A10 (Ampere Architecture)**\n",
      "    *   **Shapes:**\n",
      "        *   `VM.GPU.A10.1` (1x NVIDIA A10 GPU)\n",
      "        *   `VM.GPU.A10.2` (2x NVIDIA A10 GPUs)\n",
      "        *   `VM.GPU.A10.4` (4x NVIDIA A10 GPUs)\n",
      "    *   **Key Features:** The A10 GPUs offer a balance of performance and cost-effectiveness. They are optimized for AI inference, graphics virtualization, and mainstream deep learning training. Each A10 typically has 24GB of GDDR6 memory.\n",
      "    *   **Use Cases:** AI inference, smaller to medium deep learning model training, VDI (Virtual Desktop Infrastructure), graphic-intensive applications.\n",
      "\n",
      "3.  **NVIDIA V100 (Volta Architecture)**\n",
      "    *   **Shapes:**\n",
      "        *   `VM.GPU3.1` (1x NVIDIA V100 GPU)\n",
      "        *   `VM.GPU3.2` (2x NVIDIA V100 GPUs)\n",
      "        *   `VM.GPU3.4` (4x NVIDIA V100 GPUs)\n",
      "    *   **Key Features:** V100 GPUs were the flagship for deep learning before the Ampere generation. They feature Tensor Cores and are still highly capable for many training workloads. Each V100 GPU typically has 16GB or 32GB of HBM2 memory.\n",
      "    *   **Use Cases:** General AI/ML training, research, when A100 isn't strictly necessary, or for optimizing cost-performance.\n",
      "\n",
      "---\n",
      "\n",
      "### II. Bare Metal (BM) GPU Shapes\n",
      "\n",
      "Bare Metal GPU shapes offer direct access to the hardware for maximum performance. They are ideal for high-end distributed training clusters.\n",
      "\n",
      "1.  **NVIDIA H100 (Hopper Architecture)**\n",
      "    *   **Shape:** `BM.GPU.H100.8` (8x NVIDIA H100 GPUs)\n",
      "    *   **Key Features:** The H100 is NVIDIA's latest and most powerful GPU for AI. These instances typically feature 8x H100 GPUs, often with 80GB HBM3 memory each, NVLink 4.0 for ultra-high-speed inter-GPU communication, and the Transformer Engine for accelerating large language models (LLMs). This shape is designed for extreme-scale AI.\n",
      "    *   **Use Cases:** Training of cutting-edge large language models (LLMs), complex multi-modal AI, large-scale scientific simulations, and distributed deep learning across many nodes with ultra-low latency networking (RDMA over Converged Ethernet - RoCE).\n",
      "\n",
      "2.  **NVIDIA A100 (Ampere Architecture)**\n",
      "    *   **Shape:** `BM.GPU.A100.8` (8x NVIDIA A100 GPUs)\n",
      "    *   **Key Features:** Similar to the VM A100s, but with 8 GPUs directly connected via NVLink, offering unparalleled performance within a single server. Each A100 GPU typically has 40GB or 80GB of HBM2 memory.\n",
      "    *   **Use Cases:** Large-scale deep learning model training, AI research requiring substantial compute and memory, high-performance general-purpose GPU computing.\n",
      "\n",
      "3.  **NVIDIA V100 (Volta Architecture)**\n",
      "    *   **Shape:** `BM.GPU4.8` (8x NVIDIA V100 GPUs)\n",
      "    *   **Key Features:** This is the bare metal version of the V100, providing 8 V100 GPUs within a single server, connected by NVLink. Each V100 GPU typically has 16GB or 32GB of HBM2 memory.\n",
      "    *   **Use Cases:** High-performance AI training, large-scale HPC, when the latest generation A100/H100 is not strictly necessary or cost is a primary consideration.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Considerations When Choosing a GPU Shape:\n",
      "\n",
      "*   **Workload Type:**\n",
      "    *   **Training:** For heavy model training, especially large models, prioritize A100, H100, or V100 (BM for maximum scale).\n",
      "    *   **Inference:** A10s are very cost-effective for inference workloads. A100/V100 can also be used if higher throughput or larger batch sizes are needed.\n",
      "*   **Model Size and Complexity:** Larger models with more parameters and bigger batch sizes will require more VRAM (GPU memory) and compute power. A100s and H100s, especially the 80GB variants, excel here.\n",
      "*   **Budget:** A10s and V100s can offer better cost-performance for many use cases compared to the premium A100 and H100.\n",
      "*   **Scalability:**\n",
      "    *   If you need to scale horizontally across multiple instances for distributed training, Bare Metal instances combined with OCI's high-performance networking (RDMA) are excellent.\n",
      "    *   For scaling up within a single node, choose VM or BM instances with multiple GPUs.\n",
      "*   **NVLink:** For multi-GPU training within a single instance, NVLink is critical for fast data exchange between GPUs, drastically improving training efficiency. All A100, H100, and V100 instances with multiple GPUs leverage NVLink.\n",
      "*   **Regional Availability:** GPU resources are not available in all OCI regions, and specific shapes might have varying availability. Always check the OCI Console or documentation for the region you plan to deploy in.\n",
      "*   **Software Stack:** Ensure the chosen GPU is compatible with your AI frameworks (TensorFlow, PyTorch, etc.), CUDA version, and chosen operating system.\n",
      "\n",
      "**Practical Advice:**\n",
      "\n",
      "1.  **Check Availability:** Always verify the availability of specific GPU shapes in your desired OCI region before designing your solution.\n",
      "2.  **Start Small, Scale Up:** If unsure, begin with a smaller VM GPU instance to test your workload and then scale up (to more GPUs or a more powerful shape) as needed.\n",
      "3.  **Monitor Costs:** OCI's GPU instances can be expensive. Use OCI's cost analysis tools to monitor usage and optimize. Utilizing spot instances (preemptible instances) can also offer significant cost savings for fault-tolerant workloads.\n",
      "4.  **Leverage OCI Services:** Integrate with other OCI AI services (like OCI Data Science, OCI Vision, OCI Language) which often abstract away the underlying infrastructure even while leveraging these powerful GPUs.\n",
      "\n",
      "OCI is continually expanding its offerings, so always refer to the official OCI documentation for the most up-to-date information on available shapes and specifications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask the agent a question\n",
    "response = chat_with_agent(\n",
    "    cloud_expert_agent,\n",
    "    session_id,\n",
    "    \"What GPU shapes are available on OCI for AI workloads?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e3ae06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¤ User: How do I choose between them for training a large language model?\n",
      "\n",
      "ðŸ¤– Agent: Choosing the right OCI GPU shape for training a Large Language Model (LLM) is a critical decision that impacts performance, cost, and the feasibility of your project. \"Large\" can mean anything from fine-tuning a 7B parameter model to pre-training a 100B+ parameter model from scratch.\n",
      "\n",
      "Here's a breakdown of how to think about it, considering the unique demands of LLM training:\n",
      "\n",
      "### Key Considerations for LLM Training\n",
      "\n",
      "1.  **VRAM (GPU Memory) is Paramount:**\n",
      "    *   LLMs are memory hogs. Parameters, activations, gradients, and optimizer states all reside in VRAM.\n",
      "    *   The model \"fitting\" in memory is the first and foremost requirement. If it doesn't fit, you can't train it efficiently (or at all without complex offloading).\n",
      "    *   The optimizer choice (e.g., AdamW) significantly impacts memory usage.\n",
      "    *   Techniques like 8-bit or 4-bit quantization can reduce memory footprint but might affect precision.\n",
      "\n",
      "2.  **Compute Power (`TFLOPS`):**\n",
      "    *   Once the model fits, raw compute power (especially Tensor Core performance for mixed-precision training) determines how fast your model trains per iteration.\n",
      "\n",
      "3.  **Inter-GPU Communication (NVLink):**\n",
      "    *   If you're using multiple GPUs within a single instance, fast communication between them (NVLink) is crucial for data parallelism, model parallelism, and pipeline parallelism strategies, especially when gradients need to be shared or model layers are distributed.\n",
      "\n",
      "4.  **Inter-Node Communication (RDMA/Cluster Networking):**\n",
      "    *   For truly massive models (hundreds of billions to trillions of parameters) that require multiple 8-GPU servers, high-bandwidth, low-latency inter-node communication (like OCI's RoCEv2-enabled Cluster Networks on Bare Metal) is non-negotiable for distributed training frameworks (e.g., DeepSpeed, Megatron-LM, FSDP).\n",
      "\n",
      "5.  **Cost and Availability:**\n",
      "    *   LLM training is expensive. You need to balance performance with your budget.\n",
      "    *   Specific high-end shapes (like H100) are not available in all regions and can be in high demand.\n",
      "\n",
      "### OCI GPU Shapes for LLM Training: A Tiered Approach\n",
      "\n",
      "Let's categorize the OCI GPU shapes by their suitability for different scales of LLM training:\n",
      "\n",
      "---\n",
      "\n",
      "#### Tier 1: Entry to Mid-Level LLMs, Fine-tuning, Research & Development\n",
      "\n",
      "**Target:** Models up to ~20B parameters, intensive fine-tuning of larger models (e.g., 7B-13B), experimentation.\n",
      "\n",
      "**Shapes to Consider:**\n",
      "\n",
      "1.  **`VM.GPU.A100.4` (4x NVIDIA A100 GPUs)**\n",
      "    *   **Memory:** Up to 4x 80GB HBM2 (total 320GB) or 4x 40GB HBM2 (total 160GB). The 80GB variant is highly preferred.\n",
      "    *   **Pros:** Good balance of VRAM and compute. Uses NVLink for fast inter-GPU communication within the VM. More flexible than Bare Metal. Excellent performance for its class.\n",
      "    *   **Cons:** Still a VM, so some overhead compared to Bare Metal. Limited to 4 GPUs.\n",
      "    *   **Use Case:** Fine-tuning larger models, training smaller LLMs (e.g., up to 20B parameters depending on techniques), extensive R&D.\n",
      "\n",
      "2.  **`VM.GPU3.4` (4x NVIDIA V100 GPUs)**\n",
      "    *   **Memory:** Up to 4x 32GB HBM2 (total 128GB) or 4x 16GB HBM2 (total 64GB). The 32GB variant is strongly recommended.\n",
      "    *   **Pros:** Significantly more cost-effective than A100s. Still very capable with Tensor Cores and NVLink. Good for situations where A100s are unavailable or budget is tighter.\n",
      "    *   **Cons:** Lower compute than A100. Less VRAM per GPU. Older architecture.\n",
      "    *   **Use Case:** Fine-tuning smaller LLMs (e.g., 7B-13B), training very small LLMs from scratch, research.\n",
      "\n",
      "**Shapes generally NOT recommended for *training* large LLMs:**\n",
      "\n",
      "*   **`VM.GPU.A10.x`:** Primarily designed for inference, VDI, and graphics. While it has 24GB GDDR6, it's significantly slower for training due to lower compute and GDDR6 vs. HBM2/3 memory bandwidth. May work for *very small* LLMs or light fine-tuning, but will be much slower than A100/V100.\n",
      "*   **`VM.GPU.A100.1` / `VM.GPU3.1`:** Too few GPUs. Modern LLMs require many GPUs to fit the model and/or train efficiently with data parallelism.\n",
      "\n",
      "---\n",
      "\n",
      "#### Tier 2: Large-Scale LLM Training\n",
      "\n",
      "**Target:** Models from ~20B to 100B+ parameters, foundational model training, significant distributed training.\n",
      "\n",
      "**Shapes to Consider:**\n",
      "\n",
      "1.  **`BM.GPU.A100.8` (8x NVIDIA A100 GPUs)**\n",
      "    *   **Memory:** 8x 80GB HBM2 (total 640GB) or 8x 40GB HBM2 (total 320GB). The 80GB variant is highly, highly recommended.\n",
      "    *   **Pros:** Direct hardware access (no virtualization overhead). 8 GPUs with NVLink (NVLink 3.0) for maximum intra-node communication. Crucially, these instances are typically enabled for **OCI Cluster Networks (RDMA)**, allowing low-latency, high-bandwidth communication between multiple BM.GPU.A100.8 nodes. Essential for multi-node distributed training.\n",
      "    *   **Cons:** Higher cost. Less flexible than VMs (e.g., cannot easily resize GPU count). Availability can be limited.\n",
      "    *   **Use Case:** Training foundational models up to 100B parameters (with multi-node scaling), intensive research, high-performance distributed deep learning.\n",
      "\n",
      "2.  **`BM.GPU4.8` (8x NVIDIA V100 GPUs)**\n",
      "    *   **Memory:** 8x 32GB HBM2 (total 256GB) or 8x 16GB HBM2 (total 128GB). The 32GB variant is strongly recommended.\n",
      "    *   **Pros:** Cost-effective alternative to A100.8. Bare metal performance. 8 GPUs with NVLink (NVLink 2.0). Also typically supports OCI Cluster Networks (RDMA).\n",
      "    *   **Cons:** Lower compute and older architecture compared to A100/H100. Smaller VRAM per GPU limit the largest models unless heavy memory optimization is used.\n",
      "    *   **Use Case:** Training large models where budget is a primary constraint or A100.8/H100.8 are unavailable, or for models that fit within 256GB total VRAM (e.g., 20B-50B parameters effectively).\n",
      "\n",
      "---\n",
      "\n",
      "#### Tier 3: Cutting-Edge / Hyperscale LLM Training (The Best)\n",
      "\n",
      "**Target:** World-leading foundational models (100B+ parameters up to a trillion), complex multi-modal AI, future-proofing.\n",
      "\n",
      "**Shapes to Consider:**\n",
      "\n",
      "1.  **`BM.GPU.H100.8` (8x NVIDIA H100 GPUs)**\n",
      "    *   **Memory:** 8x 80GB HBM3 (total 640GB).\n",
      "    *   **Pros:** **The premier choice for LLMs.** Features NVIDIA's Hopper architecture, including the **Transformer Engine** specifically designed to accelerate transformer-based LLMs. Significantly higher compute (FP8/FP16) and faster memory bandwidth than A100. Uses NVLink 4.0 for even faster intra-node communication. Fully supports **OCI Cluster Networks (RDMA)**, enabling massive distributed training clusters across many nodes.\n",
      "    *   **Cons:** Highest cost. Limited availability.\n",
      "    *   **Use Case:** Pre-training cutting-edge LLMs (100B up to 1T+ parameters) from scratch, pushing state-of-the-art in AI.\n",
      "\n",
      "---\n",
      "\n",
      "### Practical Advice and Workflow:\n",
      "\n",
      "1.  **Estimate Model Size & Memory Footprint:**\n",
      "    *   **Parameters * BytesPerParameter * 4** (for FP32/FP16 parameters and gradients, and AdamW optimizer state).\n",
      "    *   A common heuristic: for FP16 training with AdamW, typically need around **20-24 bytes per parameter**.\n",
      "        *   e.g., A 7B model needs ~140-168GB.\n",
      "        *   e.g., A 70B model needs ~1.4TB - 1.68TB.\n",
      "    *   Then add activations (can be significant), input batch size, and other overhead.\n",
      "    *   This will give you a rough idea of the **total VRAM required by your model across all GPUs**.\n",
      "\n",
      "2.  **Choose Bare Metal vs. VM:**\n",
      "    *   **VM:** For R&D, fine-tuning smaller models, or when you need flexibility and don't need the absolute peak performance or multi-node scaling.\n",
      "    *   **Bare Metal:** For serious, large-scale pre-training or highly optimized fine-tuning. This\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Continue the conversation - the agent maintains context!\n",
    "response = chat_with_agent(\n",
    "    cloud_expert_agent,\n",
    "    session_id,\n",
    "    \"How do I choose between them for training a large language model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a698e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Specialized Agent Personas\n",
    "\n",
    "You can create multiple agents with different expertise for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b90a0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created specialized agents:\n",
      "   â€¢ Security Agent\n",
      "   â€¢ DevOps Agent\n"
     ]
    }
   ],
   "source": [
    "# Create a security-focused agent\n",
    "security_agent = Agent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are an OCI Security Specialist. Your expertise includes:\n",
    "- Identity and Access Management (IAM) policies\n",
    "- Network security (Security Lists, NSGs, WAF)\n",
    "- Encryption (Vault, Key Management)\n",
    "- Security Zones and Cloud Guard\n",
    "- Compliance and auditing\n",
    "\n",
    "Always emphasize security best practices and zero-trust principles.\n",
    "Warn users about potential security risks in their configurations.\n",
    "Provide specific policy examples when relevant.\"\"\",\n",
    ")\n",
    "\n",
    "# Create a DevOps agent\n",
    "devops_agent = Agent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are an OCI DevOps Engineer. Your expertise includes:\n",
    "- CI/CD pipelines with OCI DevOps\n",
    "- Container services (OKE, Container Instances)\n",
    "- Infrastructure as Code (Terraform, Resource Manager)\n",
    "- Monitoring and observability\n",
    "- Automation and scripting with OCI CLI/SDK\n",
    "\n",
    "Provide practical, automation-focused advice.\n",
    "Include code snippets and CLI commands when helpful.\n",
    "Emphasize repeatability and infrastructure as code.\"\"\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Created specialized agents:\")\n",
    "print(\"   â€¢ Security Agent\")\n",
    "print(\"   â€¢ DevOps Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c58a380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sessions created\n"
     ]
    }
   ],
   "source": [
    "# Create sessions for each agent\n",
    "security_session = security_agent.create_session(session_name=\"security_review\")\n",
    "devops_session = devops_agent.create_session(session_name=\"devops_planning\")\n",
    "\n",
    "print(\"âœ… Sessions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91391ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECURITY AGENT\n",
      "================================================================================\n",
      "\n",
      "ðŸ‘¤ User: What IAM policies do I need to set up for a team that manages AI workloads?\n",
      "\n",
      "ðŸ¤– Agent: Managing AI workloads in OCI involves a diverse set of services and requires carefully crafted IAM policies to ensure security, compliance, and optimal operational efficiency. As an OCI Security Specialist, I strongly advocate for a **zero-trust approach**, granting the **least privilege necessary** for each role and service.\n",
      "\n",
      "Here's a breakdown of recommended IAM policies, structured by typical AI team roles and OCI services, along with essential security best practices.\n",
      "\n",
      "## Core Principles for AI Workload IAM Policies\n",
      "\n",
      "1.  **Zero Trust**: Never trust, always verify. Grant the minimum permissions required for a role or service to perform its function.\n",
      "2.  **Compartment-Based Isolation**: Organize your OCI resources into logical compartments (e.g., `AI_Dev`, `AI_Test`, `AI_Prod`, `AI_DataStores`). Policies should primarily be scoped to these compartments.\n",
      "3.  **Role-Based Access Control (RBAC)**: Create specific IAM groups for different roles within your AI team (e.g., Data Scientists, MLOps Engineers, AI Admins).\n",
      "4.  **Dynamic Groups for Service-to-Service Interaction**: Use Dynamic Groups for OCI services and compute instances (e.g., Data Science Notebook Sessions, OKE Nodes) that need to interact with other OCI resources. This avoids embedding credentials.\n",
      "5.  **Resource-Specific Policies**: Whenever possible, grant permissions to specific resource types (e.g., `object-family`, `data-science-family`) rather than `all-resources`.\n",
      "6.  **Avoid `manage all-resources`**: Unless absolutely necessary for a designated super-admin account, avoid this broad permission.\n",
      "\n",
      "## Key OCI Services for AI Workloads\n",
      "\n",
      "A typical AI workload leverages several OCI services. Your policies need to cover:\n",
      "\n",
      "*   **Data Storage**: Object Storage, File Storage, Block Volume.\n",
      "*   **Compute**: Compute (VM, GPU, BM), Data Science service, Container Engine for Kubernetes (OKE).\n",
      "*   **Networking**: Virtual Cloud Network (VCN), Network Security Groups (NSG).\n",
      "*   **Identity & Access**: IAM, Dynamic Groups.\n",
      "*   **Security**: Vault, Security Zones, Cloud Guard.\n",
      "*   **Monitoring & Logging**: Monitoring, Logging, Audit.\n",
      "*   **Specialized AI Services**: Data Science service, AI Services (Language, Vision, Speech, Anomaly Detection, etc.).\n",
      "*   **Container Registry**: Oracle Container Registry (OCIR).\n",
      "\n",
      "## Recommended Compartment Structure\n",
      "\n",
      "To implement effective policies, start with a logical compartment hierarchy:\n",
      "\n",
      "```\n",
      "tenancy (root)\n",
      "â”œâ”€â”€ Production_Compartment         (for production AI models, services)\n",
      "â”œâ”€â”€ Staging_Compartment            (for UAT, pre-production deployments)\n",
      "â”œâ”€â”€ Development_Compartment        (for active development, experimentation)\n",
      "â”œâ”€â”€ DataStore_Compartment          (shared datasets, raw data - often read-only for dev/prod)\n",
      "â””â”€â”€ AI_Tools_Compartment           (shared MLOps tools, registries, CI/CD)\n",
      "```\n",
      "\n",
      "For simplicity, the examples below will refer to:\n",
      "*   `AI_DEV_COMPARTMENT`\n",
      "*   `AI_PROD_COMPARTMENT`\n",
      "*   `AI_DATA_COMPARTMENT`\n",
      "*   `AI_TOOLS_COMPARTMENT`\n",
      "\n",
      "## Recommended IAM Groups and Policies\n",
      "\n",
      "### 1. `AI_DataScientists` Group\n",
      "\n",
      "**Purpose**: For users who build, train, and experiment with AI/ML models.\n",
      "\n",
      "**Policies**:\n",
      "\n",
      "```\n",
      "# Allow Data Scientists to manage their development environment resources\n",
      "allow group AI_DataScientists to manage data-science-family in compartment AI_DEV_COMPARTMENT\n",
      "allow group AI_DataScientists to manage instance-family in compartment AI_DEV_COMPARTMENT where target.instance.shape.gpus > 0  # GPU specific instances\n",
      "allow group AI_DataScientists to manage volume-family in compartment AI_DEV_COMPARTMENT\n",
      "allow group AI_DataScientists to use virtual-network-family in compartment AI_DEV_COMPARTMENT\n",
      "\n",
      "# Access to shared data stores (often read-only or limited write)\n",
      "allow group AI_DataScientists to read object-family in compartment AI_DATA_COMPARTMENT\n",
      "allow group AI_DataScientists to manage object-family in compartment AI_DEV_COMPARTMENT\n",
      "\n",
      "# Access to secrets for API keys, external data sources\n",
      "allow group AI_DataScientists to read secret-family in compartment AI_DEV_COMPARTMENT\n",
      "allow group AI_DataScientists to use key-family in compartment AI_DEV_COMPARTMENT\n",
      "\n",
      "# View logs and metrics for their workloads\n",
      "allow group AI_DataScientists to read metrics in compartment AI_DEV_COMPARTMENT\n",
      "allow group AI_DataScientists to read logs in compartment AI_DEV_COMPARTMENT\n",
      "\n",
      "# Optional (if they need to pull/push images to OCIR)\n",
      "allow group AI_DataScientists to use repos in tenancy where request.operation='PULL'\n",
      "```\n",
      "\n",
      "**Security Notes**:\n",
      "*   Confine compute and data science project access to `AI_DEV_COMPARTMENT`.\n",
      "*   Grant `read` access to `AI_DATA_COMPARTMENT` for foundational datasets, and `manage` for their own experimental data within `AI_DEV_COMPARTMENT`.\n",
      "*   Vault access is essential for managing credentials securely.\n",
      "\n",
      "### 2. `AI_MLOps_Engineers` Group\n",
      "\n",
      "**Purpose**: For users who deploy, monitor, and maintain AI models in production.\n",
      "\n",
      "**Policies**:\n",
      "\n",
      "```\n",
      "# Manage Kubernetes clusters for model deployment\n",
      "allow group AI_MLOps_Engineers to manage cluster-family in compartment AI_PROD_COMPARTMENT\n",
      "allow group AI_MLOps_Engineers to manage virtual-network-family in compartment AI_PROD_COMPARTMENT # For OKE Networking\n",
      "allow group AI_MLOps_Engineers to use instance-family in compartment AI_PROD_COMPARTMENT # For OKE worker nodes\n",
      "\n",
      "# Manage container images in Oracle Container Registry (OCIR)\n",
      "allow group AI_MLOps_Engineers to manage repos in tenancy\n",
      "\n",
      "# Deploy and manage serverless functions for inference endpoints\n",
      "allow group AI_MLOps_Engineers to manage functions-family in compartment AI_PROD_COMPARTMENT\n",
      "\n",
      "# Access to production resources in object storage (model artifacts, inference results)\n",
      "allow group AI_MLOps_Engineers to use object-family in compartment AI_PROD_COMPARTMENT\n",
      "\n",
      "# Access to secrets and keys for production applications\n",
      "allow group AI_MLOps_Engineers to read secret-family in compartment AI_PROD_COMPARTMENT\n",
      "allow group AI_MLOps_Engineers to use key-family in compartment AI_PROD_COMPARTMENT\n",
      "\n",
      "# Full access to monitoring and logging for production workloads\n",
      "allow group AI_MLOps_Engineers to manage metrics in compartment AI_PROD_COMPARTMENT\n",
      "allow group AI_MLOps_Engineers to manage log-content in compartment AI_PROD_COMPARTMENT\n",
      "allow group AI_MLOps_Engineers to manage events-family in compartment AI_PROD_COMPARTMENT\n",
      "allow group AI_MLOps_Engineers to manage ons-topics in compartment AI_PROD_COMPARTMENT\n",
      "```\n",
      "\n",
      "**Security Notes**:\n",
      "*   Strictly limit access to `AI_PROD_COMPARTMENT`.\n",
      "*   Ensure minimal access to underlying compute resources unless explicitly required for troubleshooting OKE nodes.\n",
      "*   Strong emphasis on monitoring, logging, and events for production health.\n",
      "\n",
      "### 3. `AI_Admins` Group\n",
      "\n",
      "**Purpose**: Responsible for overall governance, infrastructure setup, and troubleshooting for the AI team.\n",
      "\n",
      "**Policies**:\n",
      "\n",
      "```\n",
      "# Manage all resources in AI team's root compartments\n",
      "allow group AI_Admins to manage all-resources in compartment AI_TEAM_ROOT_COMPARTMENT\n",
      "\n",
      "# Manage groups, dynamic groups, and policies within the tenancy for AI teams\n",
      "allow group AI_Admins to manage groups in tenancy\n",
      "allow group AI_Admins to manage dynamic-groups in tenancy\n",
      "allow group AI_Admins to manage policies in tenancy\n",
      "\n",
      "# Audit and security monitoring\n",
      "allow group AI_Admins to inspect audit-family in tenancy\n",
      "allow group AI_Admins to manage cloud-guard-family in tenancy\n",
      "allow group AI_Admins to manage security-zone-family in tenancy\n",
      "```\n",
      "\n",
      "**Security Notes**:\n",
      "*   This group has broad access; membership should be highly restricted.\n",
      "*   Enforce Multi-Factor Authentication (MFA) for this group.\n",
      "*   All actions by this group are highly scrutinized in audit logs.\n",
      "\n",
      "### 4. `AI_Auditors` Group\n",
      "\n",
      "**Purpose**: For stakeholders and compliance officers who need to view AI infrastructure and performance without modifying anything.\n",
      "\n",
      "**Policies**:\n",
      "\n",
      "```\n",
      "allow group AI_Auditors to read\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask the security agent\n",
    "print(\"=\" * 80)\n",
    "print(\"SECURITY AGENT\")\n",
    "print(\"=\" * 80)\n",
    "response = chat_with_agent(\n",
    "    security_agent,\n",
    "    security_session,\n",
    "    \"What IAM policies do I need to set up for a team that manages AI workloads?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c4369d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEVOPS AGENT\n",
      "================================================================================\n",
      "\n",
      "ðŸ‘¤ User: How do I set up a CI/CD pipeline for deploying ML models on OCI?\n",
      "\n",
      "ðŸ¤– Agent: Setting up a CI/CD pipeline for ML model deployment on OCI involves several key services working together. The goal is to automate the process from model training (or model artifact generation) to model serving in a reliable, repeatable, and scalable manner.\n",
      "\n",
      "As an OCI DevOps Engineer, I strongly advocate for **Infrastructure as Code (IaC)** using Terraform and leveraging OCI DevOps for pipeline orchestration.\n",
      "\n",
      "---\n",
      "\n",
      "### Key OCI Services Involved\n",
      "\n",
      "1.  **OCI Code Repositories (or GitHub/GitLab):** Stores your ML inference code, Dockerfiles, pipeline definitions, and deployment manifests.\n",
      "2.  **OCI Data Science (DS):**\n",
      "    *   **Model Catalog:** Centralized repository for storing, versioning, and managing ML model artifacts. Crucial for tracking model lineage.\n",
      "    *   **Model Deployments:** Managed service for deploying models as HTTP endpoints. Ideal for quick and easy serving.\n",
      "3.  **OCI Container Registry (OCIR):** Stores your Docker images for inference services.\n",
      "4.  **OCI Artifact Registry:** Can store other non-Docker artifacts, like custom model formats or dependencies.\n",
      "5.  **OCI DevOps:**\n",
      "    *   **Build Pipelines:** Automate the building of Docker images, testing, and pushing to OCIR. Can also register models to Model Catalog.\n",
      "    *   **Deploy Pipelines:** Automate the deployment of models to OCI Data Science Model Deployments, OKE, or Container Instances.\n",
      "    *   **Environments:** Define target environments (Dev, Staging, Prod).\n",
      "6.  **Oracle Container Engine for Kubernetes (OKE):** For more complex or scalable inference services, enabling microservices patterns and advanced traffic management.\n",
      "7.  **OCI Container Instances:** For simpler, cost-effective, serverless container deployments.\n",
      "8.  **OCI Object Storage:** For storing large model files, datasets, and experiment outputs. Often used as an input/output for model training.\n",
      "9.  **OCI Vault:** Securely store API keys, database credentials, or other sensitive information required by your pipeline or application.\n",
      "10. **OCI Identity and Access Management (IAM):** Define fine-grained permissions for all services and automation agents.\n",
      "11. **OCI Monitoring & Logging:** For observing your deployed models' performance, errors, and detecting drift.\n",
      "\n",
      "---\n",
      "\n",
      "### CI/CD Workflow for ML Model Deployment\n",
      "\n",
      "Here's a typical workflow, broken down into CI and CD stages, emphasizing automation:\n",
      "\n",
      "#### **Phase 0: Infrastructure Setup (Terraform - IaC)**\n",
      "\n",
      "Before building pipelines, provision your base infrastructure. This should be managed by Terraform to ensure repeatability and version control.\n",
      "\n",
      "**Example Terraform Resources:**\n",
      "*   **Compartment & IAM:**\n",
      "    ```terraform\n",
      "    resource \"oci_identity_compartment\" \"ml_devops_compartment\" {\n",
      "      name        = \"ml-devops\"\n",
      "      description = \"Compartment for ML CI/CD resources.\"\n",
      "      compartment_id = var.tenancy_ocid\n",
      "    }\n",
      "\n",
      "    resource \"oci_identity_policy\" \"devops_policy\" {\n",
      "      name          = \"ml_devops_policy\"\n",
      "      description   = \"Policy for OCI DevOps to manage resources.\"\n",
      "      compartment_id = oci_identity_compartment.ml_devops_compartment.id\n",
      "      statements    = [\n",
      "        \"Allow group ${oci_identity_group.devops_group.name} to manage repos in compartment id ${oci_identity_compartment.ml_devops_compartment.id}\",\n",
      "        \"Allow group ${oci_identity_group.devops_group.name} to manage build-pipelines in compartment id ${oci_identity_compartment.ml_devops_compartment.id}\",\n",
      "        \"Allow group ${oci_identity_group.devops_group.name} to manage deploy-pipelines in compartment id ${oci_identity_compartment.ml_devops_compartment.id}\",\n",
      "        // ... more policies for OCIR, Data Science, Object Storage, etc.\n",
      "      ]\n",
      "    }\n",
      "    ```\n",
      "*   **OCI DevOps Project:**\n",
      "    ```terraform\n",
      "    resource \"oci_devops_project\" \"ml_project\" {\n",
      "      compartment_id = oci_identity_compartment.ml_devops_compartment.id\n",
      "      name           = \"ML-Model-Deployment-Project\"\n",
      "      description    = \"DevOps project for ML model CI/CD.\"\n",
      "      namespace      = var.object_storage_namespace // Important for Build Pipeline logs\n",
      "    }\n",
      "    ```\n",
      "*   **OCI Data Science Project & Model Catalog:**\n",
      "    ```terraform\n",
      "    resource \"oci_data_science_project\" \"ml_ds_project\" {\n",
      "      compartment_id = oci_identity_compartment.ml_devops_compartment.id\n",
      "      display_name   = \"ML-Inference-Project\"\n",
      "      description    = \"Data Science Project for ML model deployments.\"\n",
      "    }\n",
      "\n",
      "    # Model Catalog is usually implicitly created with the Data Science Project,\n",
      "    # but you'll get its OCID from the project.\n",
      "    data \"oci_data_science_model_catalog\" \"ml_catalog\" {\n",
      "      project_id = oci_data_science_project.ml_ds_project.id\n",
      "    }\n",
      "    ```\n",
      "*   **OCI Container Registry (OCIR):**\n",
      "    ```terraform\n",
      "    resource \"oci_artifacts_container_repository\" \"ml_inference_repo\" {\n",
      "      compartment_id = oci_identity_compartment.ml_devops_compartment.id\n",
      "      display_name   = \"ml-inference-images\"\n",
      "      is_public      = false\n",
      "      is_immutable   = false\n",
      "      description    = \"Container repository for ML inference Docker images.\"\n",
      "    }\n",
      "    ```\n",
      "*   **OCI DevOps Code Repository (or external integration):**\n",
      "    ```terraform\n",
      "    resource \"oci_devops_repository\" \"ml_code_repo\" {\n",
      "      project_id     = oci_devops_project.ml_project.id\n",
      "      name           = \"ml-inference-code\"\n",
      "      repository_type = \"OCI_CODE_REPOSITORY\"\n",
      "      default_branch = \"main\"\n",
      "      // Optionally provide head_ssh_url, head_https_url if creating from existing\n",
      "      # mirror_repository_config {\n",
      "      #   repository_url = \"https://github.com/my-org/ml-inference.git\"\n",
      "      #   connector_id   = oci_devops_github_repository_connection.github_conn.id\n",
      "      #   trigger_builds_on_commit = true\n",
      "      # }\n",
      "    }\n",
      "    ```\n",
      "\n",
      "#### **Phase 1: Continuous Integration (CI) - OCI DevOps Build Pipeline**\n",
      "\n",
      "**Goal:** Take ML inference code, package it with the model, build a Docker image, and push it to OCIR. Optionally, register the model artifact itself into the Model Catalog.\n",
      "\n",
      "**Trigger:** Push to `main` branch, or pull request merge.\n",
      "\n",
      "**Steps:**\n",
      "\n",
      "1.  **Source Code Checkout:** Retrieve your ML inference code (Python scripts, `requirements.txt`, `Dockerfile`) from your OCI Code Repository.\n",
      "2.  **Model Artifact Retrieval:** Download the pre-trained model artifact from OCI Object Storage or a previous model training pipeline output.\n",
      "    *   **Best Practice:** The model artifact used for deployment should likely come from a *verified* model training pipeline, and its version/OCIID should be passed as a pipeline parameter or retrieved based on context (e.g., \"latest approved model\").\n",
      "3.  **Build Docker Image:** Create a Docker image that includes:\n",
      "    *   Your inference code.\n",
      "    *   Dependencies (`requirements.txt`).\n",
      "    *   The model artifact itself (often downloaded and baked into the image, or downloaded at runtime).\n",
      "    *   A simple web server (e.g., Flask, FastAPI) to expose an inference endpoint.\n",
      "4.  **Tag and Push Docker Image:** Tag the image with a unique identifier (e.g., commit SHA, pipeline run ID, version number) and push it to OCIR.\n",
      "\n",
      "**`build_spec.yaml` Example (in your code repository):**\n",
      "\n",
      "```yaml\n",
      "# build_spec.yaml\n",
      "version: 1.0\n",
      "timeoutInSeconds: 600\n",
      "inputArtifacts:\n",
      "  - artifactConfiguration:\n",
      "      source: DEVOPS_CODE_REPOSITORY\n",
      "      repositoryId: ocid1.devopsrepository.oc1..aaaa... # OCID of your OCI Code Repo\n",
      "      branch: ${OCI_BUILD_GIT_BRANCH} # Use the branch that triggered the build\n",
      "    name: ml_inference_code_repo\n",
      "outputArtifacts:\n",
      "  - name: ml_inference_image\n",
      "    type: DOCKER_IMAGE\n",
      "    # Replace with your OCIR repo path: <region>.ocir.io/<namespace>/<repo-name>\n",
      "    # The image will be tagged with build run ID and latest\n",
      "    location: \"${OCI_TENANCY_REGION}.ocir.io/${OCI_TENANCY_NAMESPACE}/ml-inference-images:${OCI_BUILD_RUN_ID}\"\n",
      "\n",
      "steps:\n",
      "  - type: Command\n",
      "    name: \"Download Model Artifact\"\n",
      "    timeoutInSeconds: 120\n",
      "    # Assuming your model is in Object Storage and you know its OCID/path\n",
      "    # The build runner must have policy to read from this bucket\n",
      "    command: |\n",
      "      MODEL_FILE=\"my_model.pkl\"\n",
      "      ML_BUCKET_NAME=\"ml-models-bucket\" # Replace with your bucket name\n",
      "      oci os object get -bn $ML_BUCKET_NAME --name $MODEL_FILE --file $MODEL_FILE --auth resource_principal\n",
      "\n",
      "  - type: Command\n",
      "    name: \"Build and Push Docker Image\"\n",
      "    timeoutInSeconds: 300\n",
      "    command: |\n",
      "      # Login to OCIR (devops build runner usually has permissions via resource principal)\n",
      "      docker login ${OCI_TENANCY_REGION}.ocir.io -u \"${OCI_TENANCY_NAMESPACE}/${OCI_DEVOPS_PROJECT_NAME}\" -p \"$(oci auth token)\"\n",
      "\n",
      "      # Get the image name from the output artifact definition\n",
      "      IMAGE_NAME=\"${OCI_TENANCY_REGION}.ocir.io/${OCI_TENANCY_NAMESPACE}/ml-inference-images\"\n",
      "      IMAGE_TAG=\"${OCI_BUILD_RUN_ID}\" # Or use ${OCI_BUILD_SOURCE_COMMIT_HASH}\n",
      "\n",
      "      # Build the Docker\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask the DevOps agent the same general area but different focus\n",
    "print(\"=\" * 80)\n",
    "print(\"DEVOPS AGENT\")\n",
    "print(\"=\" * 80)\n",
    "response = chat_with_agent(\n",
    "    devops_agent,\n",
    "    devops_session,\n",
    "    \"How do I set up a CI/CD pipeline for deploying ML models on OCI?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449ae44",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Multi-Agent Collaboration Pattern\n",
    "\n",
    "For complex tasks, you can coordinate multiple agents to work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439f42f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Multi-agent consultation function ready\n"
     ]
    }
   ],
   "source": [
    "def multi_agent_consultation(question: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get perspectives from multiple specialized agents on a question.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" MULTI-AGENT CONSULTATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n Question: {question}\\n\")\n",
    "\n",
    "    responses = {}\n",
    "\n",
    "    # Get security perspective\n",
    "    print(\"\\n SECURITY PERSPECTIVE:\")\n",
    "    print(\"-\" * 40)\n",
    "    sec_session = security_agent.create_session(session_name=\"consultation_security\")\n",
    "    response = security_agent.create_turn(\n",
    "        session_id=sec_session,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"From a security perspective: {question}\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "    sec_response = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.event.event_type == \"turn_completed\":\n",
    "            sec_response = chunk.event.final_text\n",
    "            break\n",
    "        elif chunk.event.event_type == \"step_progress\":\n",
    "            if hasattr(chunk.event.delta, 'text'):\n",
    "                print(chunk.event.delta.text, end='', flush=True)\n",
    "    responses['security'] = sec_response\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Get DevOps perspective\n",
    "    print(\"\\n DEVOPS PERSPECTIVE:\")\n",
    "    print(\"-\" * 40)\n",
    "    devops_sess = devops_agent.create_session(session_name=\"consultation_devops\")\n",
    "    response = devops_agent.create_turn(\n",
    "        session_id=devops_sess,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"From a DevOps perspective: {question}\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "    devops_response = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.event.event_type == \"turn_completed\":\n",
    "            devops_response = chunk.event.final_text\n",
    "            break\n",
    "        elif chunk.event.event_type == \"step_progress\":\n",
    "            if hasattr(chunk.event.delta, 'text'):\n",
    "                print(chunk.event.delta.text, end='', flush=True)\n",
    "    responses['devops'] = devops_response\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return responses\n",
    "\n",
    "print(\"âœ… Multi-agent consultation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c857d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " MULTI-AGENT CONSULTATION\n",
      "================================================================================\n",
      "\n",
      " Question: How should I architect a production ML inference service on OCI?\n",
      "\n",
      "\n",
      " SECURITY PERSPECTIVE:\n",
      "----------------------------------------\n",
      "Architecting a production ML inference service on OCI from a security perspective requires a robust, multi-layered approach grounded in **Zero-Trust principles**. Assume compromise at every layer and implement controls to mitigate risks.\n",
      "\n",
      "Here's a comprehensive architecture breakdown, emphasizing security best practices:\n",
      "\n",
      "## I. Core Architecture Components (Conceptual View)\n",
      "\n",
      "1.  **Client Application:** External/Internal users or applications making inference requests.\n",
      "2.  **Web Application Firewall (WAF) / DDoS Protection:** OCI WAF provides L7 protection for public endpoints.\n",
      "3.  **API Gateway:** Serves as the single, secure entry point for inference requests.\n",
      "4.  **Load Balancer:** Distributes traffic across inference endpoints.\n",
      "5.  **ML Inference Compute:** The actual service hosting the trained model.\n",
      "    *   **OCI Data Science Model Deployments:** Fully managed, highly secure, integrates well with OCI IAM. *Highly recommended for production inference.*\n",
      "    *   **Oracle Container Engine for Kubernetes (OKE):** For containerized workloads, offers great flexibility and scalability.\n",
      "    *   **Compute Instances / Container Instances:** More manual management but offers fine-grained control.\n",
      "6.  **Object Storage:** Stores ML models, potentially input data, and inference results.\n",
      "7.  **Vault:** Securely stores API keys, credentials, and encryption keys.\n",
      "8.  **Logging & Monitoring:** OCI Logging, Monitoring, Cloud Guard, OCI Audit.\n",
      "\n",
      "## II. Security Architecture Pillars\n",
      "\n",
      "### 1. Identity and Access Management (IAM)\n",
      "\n",
      "**Zero-Trust Principle:** *Verify explicitly.* Grant the least privilege necessary.\n",
      "\n",
      "*   **Principle of Least Privilege:**\n",
      "    *   **Rule:** Grant only the permissions absolutely required for a specific task or service.\n",
      "    *   **Implementation:**\n",
      "        *   **Dynamic Groups for OCI Services:** Inference compute instances (e.g., Data Science model deployments, OKE nodes, Compute VMs) should use Dynamic Groups to gain temporary, role-based access to other OCI resources (Object Storage, Vault) *without using API keys stored on the instance*.\n",
      "        *   **User Groups:** Create distinct IAM groups for SREs, ML Engineers, Auditors, and restrict their access based on their roles.\n",
      "        *   **MFA (Multi-Factor Authentication):** Enforce MFA for all human users accessing the OCI console or CLI.\n",
      "*   **Separation of Duties:**\n",
      "    *   **Rule:** No single user should have end-to-end control over critical security functions.\n",
      "    *   **Implementation:** Separate roles for network administrators, security administrators, and ML engineers.\n",
      "*   **Policy Examples:**\n",
      "\n",
      "    ```\n",
      "    # Allow ML Inference compute to read models from a specific bucket\n",
      "    Define Dynamic-Group MLInferenceDynamicGroup rule all {instance.compartment.id = 'ocid1.compartment.oc1..aaaa...'}\n",
      "\n",
      "    Allow dynamic-group MLInferenceDynamicGroup to read objects in compartment MLModelsCompartment where target.bucket.name = 'production-ml-models'\n",
      "\n",
      "    # Allow MLInference to retrieve secrets from Vault\n",
      "    Allow dynamic-group MLInferenceDynamicGroup to read secret-bundles in compartment SecretsCompartment where target.vault.id = 'ocid1.vault.oc1..aaaa...'\n",
      "\n",
      "    # Allow SREs to manage inference deployments\n",
      "    Allow group SREs to manage data-science-model-deployment in compartment MLInferenceCompartment\n",
      "\n",
      "    # Allow Auditors to read all logs\n",
      "    Allow group Auditors to read logs in tenancy\n",
      "    ```\n",
      "\n",
      "*   **Warnings & Risks:**\n",
      "    *   **Overly Permissive Policies:** `Allow group X to manage all-resources in tenancy` is a severe security risk.\n",
      "    *   **Hardcoded Credentials:** Never embed API keys or database passwords directly in your application code or Docker images. Use OCI Vault.\n",
      "    *   **Lack of MFA:** Leaves your tenancy vulnerable to credential stuffing and phishing attacks.\n",
      "\n",
      "### 2. Network Security\n",
      "\n",
      "**Zero-Trust Principle:** *Segment everywhere.* Trust no network.\n",
      "\n",
      "*   **Virtual Cloud Network (VCN) and Subnets:**\n",
      "    *   **Architecture:** Deploy your ML inference service within a **private subnet** in a dedicated VCN.\n",
      "    *   **Public vs. Private:** Never assign a public IP address to your inference compute instances.\n",
      "    *   **Ingress Control:**\n",
      "        *   **WAF:** If serving external public users, place OCI WAF in front of your API Gateway (which would have a public IP). Protects against DDoS, SQLi, XSS, etc.\n",
      "        *   **API Gateway:** Acts as a unified, rate-limited, and authenticated entry point. It should sit in a public subnet, forwarding requests to a private Load Balancer. Use TLS termination here.\n",
      "        *   **Network Security Groups (NSGs) / Security Lists:**\n",
      "            *   Apply NSGs directly to your inference compute instances, allowing *only* necessary ingress (e.g., from the Load Balancer on the inference port) and egress (e.g., to Object Storage, Vault, Logging).\n",
      "            *   **Micro-segmentation:** Use NSGs to define granular network access policies between components within the VCN.\n",
      "    *   **Egress Control:**\n",
      "        *   **Service Gateway:** Use a Service Gateway for private, secure access to OCI Public Services (Object Storage, Vault, OCI Logging) without traversing the public internet.\n",
      "        *   **NAT Gateway:** For outbound internet access (e.g., to fetch external libraries, but ideally inference services should be self-contained), use a NAT Gateway from private subnets. Restrict egress to only necessary endpoints.\n",
      "*   **TLS/SSL Everywhere:**\n",
      "    *   **Rule:** Encrypt all data in transit.\n",
      "    *   **Implementation:**\n",
      "        *   API Gateway: Enforce TLS v1.2+ for clients.\n",
      "        *   Load Balancer: Configure listeners for HTTPS.\n",
      "        *   Internal Communication: Use TLS for communication between inference service components (if applicable).\n",
      "*   **Policy Examples (NSGs):**\n",
      "\n",
      "    ```\n",
      "    # NSG for ML Inference compute instances\n",
      "    # Port 8000 for inference service, only from Load Balancer\n",
      "    Ingress Rule: Source Type: NSG (Load Balancer NSG), Protocol: TCP, Destination Port: 8000\n",
      "\n",
      "    # Allow egress to Object Storage via Service Gateway\n",
      "    Egress Rule: Destination Service: OCI-PHX-OBJECT-STORAGE (change region if needed), Protocol: All\n",
      "\n",
      "    # Allow egress to Vault via Service Gateway endpoint\n",
      "    Egress Rule: Destination Service: OCI-PHX-VAULT (change region if needed), Protocol: All\n",
      "\n",
      "    # Allow egress to Logging\n",
      "    Egress Rule: Destination Service: OCI-PHX-LOGGING (change region if needed), Protocol: All\n",
      "    ```\n",
      "*   **Warnings & Risks:**\n",
      "    *   **Open Ports:** Allowing `0.0.0.0/0` on any port other than strictly necessary (e.g., HTTPS for API Gateway) is extremely dangerous.\n",
      "    *   **Public IP Addresses on Compute:** Exposing compute instances directly to the internet.\n",
      "    *   **Lack of Egress Filtering:** Allows exfiltration of data or communication with malicious external services.\n",
      "\n",
      "### 3. Encryption\n",
      "\n",
      "**Zero-Trust Principle:** *Encrypt everything.* Protect data at rest and in transit.\n",
      "\n",
      "*   **Data at Rest:**\n",
      "    *   **Object Storage:** Configure your production model storage and potentially sensitive input/output data buckets to use **Customer-Managed Encryption Keys (CMK)** from OCI Vault. This gives you full control over the key lifecycle.\n",
      "    *   **Boot/Block Volumes:** Encrypt all boot and block volumes used by inference compute instances with CMKs.\n",
      "    *   **Database (if used):** If your ML service relies on a database for feature stores or metadata, ensure it's encrypted at rest using CMKs (e.g., Autonomous Database, Database Service).\n",
      "*   **Data in Transit:**\n",
      "    *   **TLS/SSL:** As mentioned in Network Security, enforce TLS v1.2+ for all communication, both external and internal.\n",
      "*   **Secrets Management:**\n",
      "    *   **OCI Vault:** Use OCI Vault to securely store all sensitive credentials (API keys for third-party services, database passwords, private keys, etc.).\n",
      "    *   **Integration:** Your inference service should retrieve secrets from Vault at runtime using its Dynamic Group identity, not store them locally.\n",
      "*   **Warnings & Risks:**\n",
      "    *   **Default Oracle-managed Keys:** While better than nothing, CMKs offer superior control and meet higher compliance requirements.\n",
      "    *   **Exposed Secret Bundles:** Ensure IAM policies for Vault secrets are tightly constrained.\n",
      "    *   **Unencrypted Traffic:** Intercepted data is readable if not encrypted.\n",
      "\n",
      "### 4. Security Zones and Cloud Guard\n",
      "\n",
      "**Zero-Trust Principle:** *Automate security operations.* Detect and respond to threats automatically.\n",
      "\n",
      "*   **OCI Security Zones:**\n",
      "    *   **Recommendation:** Place your ML inference compartment (where models, deployments, and compute reside) within an **OCI Security Zone**.\n",
      "    *   **Enforcement:** Security Zones *proactively enforce* OCI security best practices by preventing actions that violate security policies (e.g., creating public buckets, attaching public IPs, using default encryption). This is a powerful preventative control.\n",
      "    *   **Benefits:** Guarantees that sensitive production assets adhere to a strict security posture from creation.\n",
      "*   **OCI Cloud Guard:**\n",
      "    *   **Monitoring & Detection:** Configure Cloud Guard to monitor your tenancy, especially the ML inference compartment.\n",
      "    *   **Threat Detection:** Identifies misconfigurations, suspicious activity, and potential threats.\n",
      "    *   **Responder Recipes:** Automate responses to detected threats (e.g., quarantining an instance, blocking network traffic).\n",
      "*   **Warnings & Risks:**\n",
      "    *   **Ignoring Cloud Guard Alerts:** Leads to undetected compromises and configuration drift.\n",
      "    *   **Not using Security Zones:** Allows engineers to accidentally or intentionally create insecure configurations that violate best practices.\n",
      "\n",
      "### 5. Compliance and Auditing\n",
      "\n",
      "**Zero-Trust Principle:** *Visibility is key.* Log everything, verify policies regularly.\n",
      "\n",
      "*   **OCI Audit:**\n",
      "    *   **Enabled by Default:** OCI Audit logs all API activity across your tenancy.\n",
      "    *   **Monitoring:** Integrate OCI Audit logs into your centralized logging solution and monitor for suspicious activities (e.g., unauthorized policy changes, attempts to delete critical resources).\n",
      "*   **OCI Logging Service:**\n",
      "    *   **Centralization:** Aggregate all logs from your inference compute (application logs, system logs, web server logs) into OCI Logging.\n",
      "    *   **Alerting:** Configure alarms on specific log patterns indicative of security incidents (e.g., failed authentication attempts, high error rates, unauthorized access attempts).\n",
      "*   **Cloud Guard:**\n",
      "    *   **Continuous Compliance:** Cloud Guard continuously checks resource configurations against security benchmarks and best practices, helping maintain compliance posture.\n",
      "*   **Monitoring & Alarms:**\n",
      "    *   **OCI Monitoring:** Set up alarms"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " DEVOPS PERSPECTIVE:\n",
      "----------------------------------------\n",
      "As an OCI DevOps Engineer, architecting a production ML inference service on OCI demands a focus on **automation, repeatability, scalability, and observability**. We want to treat our ML models and their serving infrastructure as code, integrate them into CI/CD pipelines, and ensure robust monitoring.\n",
      "\n",
      "Here's a comprehensive approach, emphasizing OCI services and DevOps best practices.\n",
      "\n",
      "## Core Principles for Production ML Inference\n",
      "\n",
      "1.  **Infrastructure as Code (IaC):** Provision and manage all infrastructure with Terraform via OCI Resource Manager. This ensures repeatability and version control.\n",
      "2.  **Containerization:** Package your ML model and its serving logic into Docker containers. This provides environment consistency.\n",
      "3.  **CI/CD:** Automate the build, test, and deployment of your model and its inference service using OCI DevOps.\n",
      "4.  **Model Registry:** Store, version, and manage your trained models securely.\n",
      "5.  **Observability:** Implement comprehensive monitoring, logging, and alerting to understand service health and model performance.\n",
      "6.  **Security First:** Integrate security from design to deployment.\n",
      "7.  **Scalability & High Availability:** Design for resilience and to handle varying load.\n",
      "\n",
      "## 1. Choosing Your Deployment Target\n",
      "\n",
      "OCI offers several robust options for deploying ML inference services. Your choice depends on needed flexibility, operational overhead, and integration with the broader ML lifecycle.\n",
      "\n",
      "### Option A: OCI Data Science Model Deployments (Recommended for Pure ML Inference)\n",
      "\n",
      "This is OCI's managed service specifically designed for deploying ML models. It streamlines the process for data scientists and provides built-in MLOps features.\n",
      "\n",
      "*   **Pros:**\n",
      "    *   **Fully Managed:** OCI handles the underlying infrastructure (compute, networking, scaling).\n",
      "    *   **Simplified Deployment:** Data scientists can deploy models directly from the Model Catalog.\n",
      "    *   **Built-in MLOps:** Integrates with Model Catalog, logging, monitoring, and auto-scaling.\n",
      "    *   **Reduced Operational Overhead:** Less infrastructure code and maintenance.\n",
      "*   **Cons:**\n",
      "    *   Less control over the underlying compute and networking than OKE.\n",
      "    *   May be less flexible for highly custom serving frameworks or complex microservices architectures.\n",
      "\n",
      "#### DevOps Perspective for OCI Data Science Model Deployments\n",
      "\n",
      "Your CI/CD pipeline would focus on:\n",
      "1.  **Model Training & Evaluation:** (Often outside the inference CI/CD, but triggers it).\n",
      "2.  **Model Packaging:** Create a `runtime.yaml` and `score.py` file alongside your model artifact.\n",
      "3.  **Model Registration:** Upload the model artifact to OCI Object Storage, then register it in the OCI Data Science Model Catalog.\n",
      "4.  **Model Deployment:** Create/update an OCI Data Science Model Deployment, referencing the registered model.\n",
      "\n",
      "**Example Terraform for Data Science Model Deployment:**\n",
      "\n",
      "```terraform\n",
      "resource \"oci_data_science_model_deployment\" \"ml_inference_service\" {\n",
      "  compartment_id = var.compartment_ocid\n",
      "  project_id     = oci_data_science_project.ml_project.id\n",
      "  display_name   = \"my-ml-inference-service\"\n",
      "  description    = \"Production inference service for X model\"\n",
      "\n",
      "  category_log_details {\n",
      "    access_log { # Optional: Enable access logs for the endpoint\n",
      "      log_group_id = oci_logging_log_group.inference_log_group.id\n",
      "      log_id       = oci_logging_log.inference_access_log.id\n",
      "    }\n",
      "    predict_log { # Optional: Enable prediction logs for model output\n",
      "      log_group_id = oci_logging_log_group.inference_log_group.id\n",
      "      log_id       = oci_logging_log.inference_predict_log.id\n",
      "    }\n",
      "  }\n",
      "\n",
      "  model_deployment_configuration_details {\n",
      "    deployment_type = \"SINGLE_MODEL\"\n",
      "    model_configuration_details {\n",
      "      model_id                     = local.model_id_to_deploy # Referencing a model from Model Catalog\n",
      "      instance_configuration {\n",
      "        instance_shape_name        = \"VM.Standard.E4.Flex\" # Choose appropriate shapes\n",
      "        ocpus                      = 1\n",
      "        memory_in_gbs              = 16\n",
      "      }\n",
      "      bandwidth_mbps = 50 # Optional: Configure network bandwidth\n",
      "      scaling_policy {\n",
      "        auto_scaling_policy_type = \"FIXED_SIZE\" # Can use \"THRESHOLD\" for auto-scaling\n",
      "        instance_count           = 1            # Starting instance count\n",
      "      }\n",
      "      # Update environment variables or other settings if needed\n",
      "    }\n",
      "  }\n",
      "\n",
      "  # Example if using THRESHOLD scaling policy\n",
      "  # scaling_policy {\n",
      "  #   auto_scaling_policy_type = \"THRESHOLD\"\n",
      "  #   instance_count           = 1\n",
      "  #   auto_scaling_model_deployment_update_policy {\n",
      "  #     warmup_duration_in_ms = \"300000\"\n",
      "  #   }\n",
      "  #   auto_scaling_policy_details {\n",
      "  #     auto_scaling_init_duration_in_ms = \"300000\"\n",
      "  #     auto_scaling_resources {\n",
      "  #       resource = \"INSTANCE\"\n",
      "  #     }\n",
      "  #     policies {\n",
      "  #       policy_type = \"THRESHOLD\"\n",
      "  #       rules {\n",
      "  #         metric_type = \"CPU_UTILIZATION\"\n",
      "  #         scale_in_configurations {\n",
      "  #           operator = \"LT\"\n",
      "  #           value    = var.cpu_scale_in_threshold # e.g., 20\n",
      "  #           scale_in_configuration_type = \"FIXED_SIZE\"\n",
      "  #           num_instance             = 1\n",
      "  #         }\n",
      "  #         scale_out_configurations {\n",
      "  #           operator = \"GT\"\n",
      "  #           value    = var.cpu_scale_out_threshold # e.g., 70\n",
      "  #           scale_out_configuration_type = \"FIXED_SIZE\"\n",
      "  #           num_instance             = 1\n",
      "  #         }\n",
      "  #       }\n",
      "  #     }\n",
      "  #   }\n",
      "  # }\n",
      "}\n",
      "\n",
      "# Assume oci_data_science_project, oci_logging_log_group, oci_logging_log are defined elsewhere\n",
      "```\n",
      "\n",
      "You would orchestrate the `local.model_id_to_deploy` by a previous pipeline step outputting the OCID of the newly registered model.\n",
      "\n",
      "### Option B: OCI Container Engine for Kubernetes (OKE)\n",
      "\n",
      "For maximum flexibility, control, and integration into existing microservices architectures, OKE is an excellent choice.\n",
      "\n",
      "*   **Pros:**\n",
      "    *   **Full Control:** Fine-tuned control over compute, networking, and deployment strategies (Blue/Green, Canary).\n",
      "    *   **Scalability:** Advanced auto-scaling (HPA, Cluster Autoscaler) and self-healing capabilities.\n",
      "    *   **Ecosystem:** Leverage the vast Kubernetes ecosystem (Helm, Istio, Prometheus, Grafana).\n",
      "    *   **Complex Workloads:** Ideal for multi-model deployments or where custom Triton Inference Server setups are needed.\n",
      "*   **Cons:**\n",
      "    *   Higher operational overhead; requires Kubernetes expertise.\n",
      "    *   More infrastructure code to manage (Terraform for OKE, K8s manifests, Helm charts).\n",
      "\n",
      "#### DevOps Perspective for OKE-based Deployment\n",
      "\n",
      "Your CI/CD pipeline would involve:\n",
      "1.  **Model Training & Packaging:** Model artifact saved.\n",
      "2.  **Container Image Build:** Dockerize your inference API (e.g., FastAPI with your model, or a full Triton Inference Server image).\n",
      "    *   Ensure the container can load the model from OCI Object Storage/Model Catalog.\n",
      "    *   Push engineered container images to OCI Container Registry (OCIR).\n",
      "3.  **Kubernetes Deployment:**\n",
      "    *   Terraform provisions/updates the OKE cluster (if needed).\n",
      "    *   Deployment stage applies Kubernetes manifests or Helm charts to OKE.\n",
      "\n",
      "**Example Terraform for OKE Cluster:**\n",
      "\n",
      "```terraform\n",
      "resource \"oci_containerengine_cluster\" \"ml_inference_cluster\" {\n",
      "  compartment_id     = var.compartment_ocid\n",
      "  vcn_id             = oci_core_vcn.ml_vcn.id\n",
      "  kubernetes_version = var.kubernetes_version\n",
      "  name               = \"ml-inference-cluster\"\n",
      "  options {\n",
      "    kubernetes_dashboard                 = { is_enabled = false } # Disable dashboard in prod\n",
      "    tiller                               = { is_enabled = false }\n",
      "    service_lb_subnet_ids                = [oci_core_subnet.public_lb_subnet.id]\n",
      "    # Add other advanced options like network configuration etc.\n",
      "  }\n",
      "}\n",
      "\n",
      "resource \"oci_containerengine_node_pool\" \"ml_inference_nodepool\" {\n",
      "  cluster_id         = oci_containerengine_cluster.ml_inference_cluster.id\n",
      "  compartment_id     = var.compartment_ocid\n",
      "  kubernetes_version = var.kubernetes_version\n",
      "  name               = \"ml-inference-nod\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run a multi-agent consultation\n",
    "results = multi_agent_consultation(\n",
    "    \"How should I architect a production ML inference service on OCI?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4893b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Agentic Workflow: Infrastructure Review\n",
    "\n",
    "Let's add an architect agent that reviews the responses of the security and devops agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a0bee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created Architect Agent\n"
     ]
    }
   ],
   "source": [
    "# Create an architect agent that synthesizes recommendations\n",
    "architect_agent = Agent(\n",
    "    client=client,\n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are a Senior OCI Solutions Architect. Your role is to:\n",
    "- Review infrastructure requirements and constraints\n",
    "- Synthesize input from security and DevOps perspectives\n",
    "- Provide comprehensive, balanced architectural recommendations\n",
    "- Consider cost, performance, security, and operational aspects\n",
    "- Create actionable implementation plans\n",
    "\n",
    "You provide executive-level summaries followed by detailed technical guidance.\"\"\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Created Architect Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b45531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Infrastructure review workflow ready\n"
     ]
    }
   ],
   "source": [
    "def infrastructure_review_workflow(requirements: str):\n",
    "    \"\"\"\n",
    "    A multi-stage workflow for infrastructure review.\n",
    "\n",
    "    Stage 1: Security Analysis\n",
    "    Stage 2: DevOps Analysis\n",
    "    Stage 3: Architecture Synthesis\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"  INFRASTRUCTURE REVIEW WORKFLOW\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n Requirements:\\n{requirements}\\n\")\n",
    "\n",
    "    # Stage 1: Security Analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" STAGE 1: Security Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    sec_session = security_agent.create_session(session_name=\"review_security\")\n",
    "    sec_response = security_agent.create_turn(\n",
    "        session_id=sec_session,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Review these requirements for security considerations:\\n\\n{requirements}\\n\\nProvide key security requirements and concerns.\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "    security_analysis = \"\"\n",
    "    for chunk in sec_response:\n",
    "        if chunk.event.event_type == \"turn_completed\":\n",
    "            security_analysis = chunk.event.final_text\n",
    "            break\n",
    "        elif chunk.event.event_type == \"step_progress\":\n",
    "            if hasattr(chunk.event.delta, 'text'):\n",
    "                print(chunk.event.delta.text, end='', flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Stage 2: DevOps Analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" STAGE 2: DevOps Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    devops_sess = devops_agent.create_session(session_name=\"review_devops\")\n",
    "    devops_response = devops_agent.create_turn(\n",
    "        session_id=devops_sess,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Review these requirements for DevOps implementation:\\n\\n{requirements}\\n\\nProvide key operational requirements and automation opportunities.\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "    devops_analysis = \"\"\n",
    "    for chunk in devops_response:\n",
    "        if chunk.event.event_type == \"turn_completed\":\n",
    "            devops_analysis = chunk.event.final_text\n",
    "            break\n",
    "        elif chunk.event.event_type == \"step_progress\":\n",
    "            if hasattr(chunk.event.delta, 'text'):\n",
    "                print(chunk.event.delta.text, end='', flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Stage 3: Architecture Synthesis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" STAGE 3: Architecture Synthesis\")\n",
    "    print(\"=\"*80)\n",
    "    arch_session = architect_agent.create_session(session_name=\"review_synthesis\")\n",
    "    synthesis_prompt = f\"\"\"Based on the following inputs, provide a comprehensive infrastructure recommendation:\n",
    "\n",
    "## Original Requirements:\n",
    "{requirements}\n",
    "\n",
    "## Security Analysis:\n",
    "{security_analysis}\n",
    "\n",
    "## DevOps Analysis:\n",
    "{devops_analysis}\n",
    "\n",
    "Provide a synthesized recommendation that addresses all concerns with a clear implementation plan.\"\"\"\n",
    "\n",
    "    arch_response = architect_agent.create_turn(\n",
    "        session_id=arch_session,\n",
    "        messages=[{\"role\": \"user\", \"content\": synthesis_prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    final_recommendation = \"\"\n",
    "    for chunk in arch_response:\n",
    "        if chunk.event.event_type == \"turn_completed\":\n",
    "            final_recommendation = chunk.event.final_text\n",
    "            break\n",
    "        elif chunk.event.event_type == \"step_progress\":\n",
    "            if hasattr(chunk.event.delta, 'text'):\n",
    "                print(chunk.event.delta.text, end='', flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… REVIEW COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return {\n",
    "        \"security_analysis\": security_analysis,\n",
    "        \"devops_analysis\": devops_analysis,\n",
    "        \"final_recommendation\": final_recommendation\n",
    "    }\n",
    "\n",
    "print(\"âœ… Infrastructure review workflow ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c0f9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  INFRASTRUCTURE REVIEW WORKFLOW\n",
      "================================================================================\n",
      "\n",
      " Requirements:\n",
      "\n",
      "We need to deploy a real-time ML inference service with the following requirements:\n",
      "\n",
      "1. Handle 1000 requests per second at peak\n",
      "2. Maximum latency of 100ms for 99th percentile\n",
      "3. Use GPU-accelerated inference\n",
      "4. Support model updates without downtime\n",
      "5. Store inference logs for 90 days\n",
      "6. Operate in us-phoenix-1 region\n",
      "7. Budget: ~$10,000/month\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      " STAGE 1: Security Analysis\n",
      "================================================================================\n",
      "As an OCI Security Specialist, I've reviewed your requirements for a real-time ML inference service. Deploying a high-performance, real-time system like this, especially with GPU acceleration and continuous updates, introduces several critical security considerations. Our focus will be on embedding security from design through operations, adhering to **Zero-Trust principles** and **least privilege**, and utilizing OCI's robust security capabilities.\n",
      "\n",
      "Here are the key security requirements and concerns, categorized for clarity:\n",
      "\n",
      "---\n",
      "\n",
      "### Core Security Principles\n",
      "\n",
      "Before diving into specifics, let's reiterate fundamental principles:\n",
      "\n",
      "1.  **Zero-Trust:** Never trust, always verify. Every request, every user, every non-human identity, and every network connection must be authenticated, authorized, and continuously validated.\n",
      "2.  **Least Privilege:** Grant only the minimum necessary permissions to perform a task for both human and non-human identities.\n",
      "3.  **Defense-in-Depth:** Employ multiple layers of security controls to protect against failure at any single point.\n",
      "4.  **Proactive Security:** Use tools like OCI Cloud Guard and Security Zones to prevent misconfigurations and detect threats dynamically.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Security Requirements & Concerns\n",
      "\n",
      "#### A. Identity and Access Management (IAM)\n",
      "\n",
      "*   **Concern:** Unauthorized access to inference endpoints, model repositories, log data, or the underlying compute infrastructure.\n",
      "*   **Requirement:**\n",
      "    *   **Principle of Least Privilege:** Strictly define user groups and assign granular IAM policies for every interaction:\n",
      "        *   **Inference Service Identity:** The Compute instances running the ML model should have a dynamic group and an IAM policy that *only* allows them to perform necessary actions (e.g., pulling models from Object Storage, pushing logs to Object Storage, updating monitoring metrics).\n",
      "        *   **Model Deployment Identity:** A separate identity (e.g., CI/CD pipeline service account) with policies *only* to push new models to the secure model repository and manage target Compute instances (e.g., rolling updates).\n",
      "        *   **Log Access Identity:** Specific groups for security analysts or data scientists to access **encrypted** inference logs in Object Storage, with read-only access.\n",
      "        *   **Administrator Identity:** Highly privileged access, requiring Multi-Factor Authentication (MFA) and ideally OCI Identity Governance for privilege access management.\n",
      "    *   **Compartment Strategy:** Isolate environments (e.g., `ml-prod`, `ml-dev`, `ml-security`). Use a dedicated compartment for the production ML service and another for sensitive data like logs and models.\n",
      "    *   **MFA Enforcement:** Mandate MFA for all user accounts, especially administrators.\n",
      "    *   **Session Management:** Implement short-lived session tokens for all programmatic access.\n",
      "\n",
      "*   **Security Risk:** Overly permissive policies, shared credentials, or lack of MFA can lead to compromise of the inference service, data exfiltration, or model poisoning.\n",
      "*   **Zero-Trust Aspect:** Every human and non-human entity interacting with the service must have explicit, time-bound, and least-privileged authorization.\n",
      "\n",
      "*   **Example Policy for Inference Service (Dynamic Group 'ml_inference_instances'):**\n",
      "    ```\n",
      "    Allow dynamic-group ml_inference_instances to read objects in compartment MLResources where target.bucket.name = 'ml_models_repo'\n",
      "    Allow dynamic-group ml_inference_instances to manage object-family in compartment MLLogs where target.bucket.name = 'ml_inference_logs'\n",
      "    Allow dynamic-group ml_inference_instances to use vnics in compartment MLResources\n",
      "    Allow dynamic-group ml_inference_instances to inspect metrics in compartment MLResources\n",
      "    ```\n",
      "\n",
      "#### B. Network Security\n",
      "\n",
      "*   **Concern:** Public exposure of the inference endpoint, unauthorized network access to GPU instances, data exfiltration via network.\n",
      "*   **Requirement:**\n",
      "    *   **VCN Segmentation:** Deploy resources in a Virtual Cloud Network (VCN) with private subnets for your GPU instances. **Never expose compute instances directly to the internet.**\n",
      "    *   **Load Balancer:** Use an OCI Load Balancer to distribute traffic to your inference service, ensuring high availability and protecting backend instances.\n",
      "    *   **Web Application Firewall (WAF):** Place OCI WAF in front of your Load Balancer for critical threat protection:\n",
      "        *   **DDoS Protection:** Mitigate potential Layer 3/4/7 DDoS attacks targeting your real-time service.\n",
      "        *   **API Security:** Protect the inference API endpoint from SQL injection, cross-site scripting (although less common for ML APIs, still possible), and other OWASP Top 10 threats.\n",
      "        *   **Rate Limiting:** Protect against abuse and ensure service availability under high load, especially crucial for 1000 req/sec peak.\n",
      "        *   **Bot Management:** Identify and block malicious bots or scraping attempts.\n",
      "    *   **Network Security Groups (NSGs) / Security Lists:** Apply strict ingress/egress rules:\n",
      "        *   **Ingress:** Only allow traffic to the GPU instances from the Load Balancer (and potentially management subnets).\n",
      "        *   **Egress:** Only allow necessary outbound connections (e.g., to OCI Object Storage for models/logs, KMS for encryption, OCI Vault for secrets, monitoring endpoints). Block all other outbound traffic.\n",
      "        *   **Micro-segmentation:** Use NSGs to control traffic *between* your GPU instances, and between different tiers of your architecture.\n",
      "    *   **Service Gateway / Private Endpoints:** Access OCI services like Object Storage (for models and logs) and Vault (for keys/secrets) directly and privately from your VCN, without traversing the public internet. This enhances data security and compliance.\n",
      "\n",
      "*   **Security Risk:** Lack of WAF for DDoS, open network ports, or inadequate segmentation can lead to service disruption, unauthorized access, or data breaches.\n",
      "*   **Zero-Trust Aspect:** Explicitly define and restrict all network paths and access points. All traffic should be inspected and authorized.\n",
      "\n",
      "*   **Example NSG Rules for GPU Instance (Ingress for Inference Port):**\n",
      "    ```\n",
      "    Rule 1 (Ingress): Source Type: CIDR, Source CIDR: <Load Balancer Private IP Range>, IP Protocol: TCP, Destination Port: <Inference Service Port> -> Allow\n",
      "    Rule 2 (Ingress): Source Type: CIDR, Source CIDR: <Management Subnet CIDR (for SSH)>, IP Protocol: TCP, Destination Port: 22 -> Allow (only if direct SSH is needed, prefer Bastion Service)\n",
      "    Rule 3 (Egress): Destination Type: Service, Service: All <region> services, IP Protocol: All -> Allow (via Service Gateway)\n",
      "    Rule 4 (Egress): Destination Type: CIDR, Destination CIDR: 0.0.0.0/0, IP Protocol: All -> Deny (unless specific external APIs are needed, then list them explicitly)\n",
      "    ```\n",
      "\n",
      "#### C. Data Protection (Encryption & Integrity)\n",
      "\n",
      "*   **Concern:** Data breaches of sensitive inference input/output, compromise of trained models, unauthorized access to encryption keys or application secrets.\n",
      "*   **Requirement:**\n",
      "    *   **Encryption at Rest:** Ensure all data is encrypted:\n",
      "        *   **GPU Instance Boot/Block Volumes:** Encrypted by default. **Strongly recommend using Customer-Managed Keys (CMK) from OCI Vault/KMS** for enhanced control and compliance.\n",
      "        *   **Inference Logs (Object Storage):** Encrypted at rest. Use CMK from OCI Vault/KMS for maximum security.\n",
      "        *   **ML Models (Object Storage):** Encrypted at rest. Use CMK from OCI Vault/KMS.\n",
      "    *   **Encryption in Transit:** All communications must use TLS/SSL:\n",
      "        *   **Client to WAF/Load Balancer"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      " STAGE 2: DevOps Analysis\n",
      "================================================================================\n",
      "This is an excellent set of requirements for a real-time ML inference service, highlighting critical concerns like performance, availability, cost, and maintainability. As an OCI DevOps Engineer, my advice will focus on leveraging OCI services, Infrastructure as Code (IaC), and CI/CD best practices to meet these goals sustainably.\n",
      "\n",
      "---\n",
      "\n",
      "## Executive Summary: Recommended OCI Architecture\n",
      "\n",
      "The core architecture will revolve around **OCI Container Engine for Kubernetes (OKE)** running GPU-enabled instances, integrated with **OCI DevOps** for CI/CD, **OCI Load Balancer** for external access, and robust **OCI Monitoring** and **Logging** for observability. All infrastructure will be defined and managed using **Terraform** via **OCI Resource Manager**.\n",
      "\n",
      "---\n",
      "\n",
      "## Detailed Requirements Breakdown & OCI Solutions\n",
      "\n",
      "### 1. Handle 1000 requests per second at peak\n",
      "\n",
      "*   **OCI Solution:** **Oracle Container Engine for Kubernetes (OKE)** with **Horizontal Pod Autoscaling (HPA)** and **Cluster Autoscaling**.\n",
      "    *   OKE provides a highly scalable and resilient platform for containerized applications.\n",
      "    *   **HPA**: Automatically scales the number of inference pods based on CPU, memory, or custom metrics (e.g., requests per second, GPU utilization).\n",
      "    *   **Cluster Autoscaler**: Automatically provisions and de-provisions OKE worker nodes (including GPU instances) based on pod pending state, ensuring the cluster can always handle the load.\n",
      "    *   **OCI Load Balancer**: Distributes incoming traffic across your OKE ingress or service endpoints.\n",
      "*   **Operational Requirements:**\n",
      "    *   **Load Testing:** Absolutely critical. Simulate peak loads to validate HPA and Cluster Autoscaler configurations and identify bottlenecks.\n",
      "    *   **Resource Limits & Requests:** Properly define CPU and memory limits/requests for inference pods to ensure efficient scheduling and resource utilization.\n",
      "    *   **Network Capacity:** Ensure the VCN and Subnets have sufficient CIDR blocks and network throughput.\n",
      "*   **Automation Opportunities:**\n",
      "    *   **IaC for OKE:** Use Terraform to define the OKE cluster, node pools (including GPU ones), Load Balancers, and network components.\n",
      "    *   **Automated HPA/Cluster Autoscaler Configuration:** Define HPA rules and Cluster Autoscaler parameters in Kubernetes manifests, managed by your OCI DevOps deployment pipeline.\n",
      "\n",
      "```terraform\n",
      "# Terraform snippet for an OKE Cluster with a GPU node pool\n",
      "resource \"oci_containerengine_cluster\" \"ml_inference_cluster\" {\n",
      "  # ... cluster configuration ...\n",
      "}\n",
      "\n",
      "resource \"oci_containerengine_node_pool\" \"gpu_node_pool\" {\n",
      "  cluster_id   = oci_containerengine_cluster.ml_inference_cluster.id\n",
      "  compartment_id = var.compartment_ocid\n",
      "  name           = \"gpu-inference-pool\"\n",
      "  kubernetes_version = oci_containerengine_cluster.ml_inference_cluster.kubernetes_version\n",
      "  node_shape     = \"VM.GPU.A10.1\" # Example GPU shape\n",
      "  node_shape_config {\n",
      "    ocpus         = 15 # Match VM.GPU.A10.1 OCPUs, or size for specific GPU usage\n",
      "    memory_in_gbs = 240 # Match VM.GPU.A10.1 memory\n",
      "  }\n",
      "  node_config_details {\n",
      "    placement_configs {\n",
      "      availability_domain = data.oci_identity_availability_domains.ads.availability_domains[0].name\n",
      "      subnet_id           = oci_core_subnet.oke_worker_subnet.id\n",
      "      # For high availability, consider multiple ADs or Flex ADs\n",
      "    }\n",
      "    size = 1 # Start with a small size, Cluster Autoscaler will manage\n",
      "  }\n",
      "  initial_node_labels = {\n",
      "    \"accelerator\" = \"gpu\"\n",
      "  }\n",
      "  defined_tags = var.common_defined_tags # For cost tracking\n",
      "  # ... other node pool configuration ...\n",
      "}\n",
      "```\n",
      "\n",
      "### 2. Maximum latency of 100ms for 99th percentile\n",
      "\n",
      "*   **OCI Solution:**\n",
      "    *   **OKE with optimized network and instance types:** Running inference models efficiently.\n",
      "    *   **OCI FastConnect/VPN Connect (if clients are on-premise):** Reduce network latency to the cloud.\n",
      "    *   **Private Endpoints for internal services:** If the model depends on other OCI services (e.g., database, private API), use private endpoints to keep traffic within the VCN.\n",
      "    *   **Dedicated Load Balancer:** Avoid shared resources where possible.\n",
      "    *   **Application-level optimization:** Efficient model serving framework (e.g., NVIDIA Triton Inference Server, TorchServe, TensorFlow Serving), optimized Docker images, model quantization.\n",
      "*   **Operational Requirements:**\n",
      "    *   **Critical Path Analysis:** Identify every component in the request path (client network, Load Balancer, OKE Ingress, Pod network, inference execution) and measure its latency.\n",
      "    *   **Monitoring:** Use OCI Monitoring with custom metrics for P99 latency within the application and at the Load Balancer level. OCI APM can provide granular trace data.\n",
      "    *   **Network Topology:** Deploy OKE in private subnets, use Network Security Groups (NSGs) for fine-grained traffic control but ensure they don't introduce overhead.\n",
      "*   **Automation Opportunities:**\n",
      "    *   **Automated Performance Testing:** Integrate latency tests into your CI/CD pipeline to detect regressions.\n",
      "    *   **Alerting on P99 Latency:** Configure OCI Alarms to trigger if latency thresholds are breached. PagerDuty/Slack integration for critical alerts.\n",
      "\n",
      "```yaml\n",
      "# Kubernetes HPA example targeting custom metric (e.g., requests per second)\n",
      "apiVersion: autoscaling/v2beta2\n",
      "kind: HorizontalPodAutoscaler\n",
      "metadata:\n",
      "  name: ml-inference-hpa\n",
      "spec:\n",
      "  scaleTargetRef:\n",
      "    apiVersion: apps/v1\n",
      "    kind: Deployment\n",
      "    name: ml-inference-service\n",
      "  minReplicas: 2\n",
      "  maxReplicas: 10\n",
      "  metrics:\n",
      "  - type: Pods\n",
      "    pods:\n",
      "      metricName: inference_qps # Custom metric exposed by your application\n",
      "      targetAverageValue: 500m # Target 0.5 requests/second per pod (adjust based on profiling)\n",
      "  # Or target CPU utilization based on profiling\n",
      "  # - type: Resource\n",
      "  #   resource:\n",
      "  #     name: cpu\n",
      "  #     target:\n",
      "  #       type: Utilization\n",
      "  #       averageUtilization: 80\n",
      "```\n",
      "\n",
      "### 3. Use GPU-accelerated inference\n",
      "\n",
      "*   **OCI Solution:**\n",
      "    *   **OKE with GPU-enabled VM instances:** OCI offers various GPU shapes (e.g., VM.GPU.A10.1, VM.GPU.A10.2, BM.GPU.A10.4). Select the most cost-effective one based on your model's specific requirements.\n",
      "    *   **NVIDIA GPU Operator for Kubernetes:** Simplifies GPU device plugin installation and management within OKE.\n",
      "    *   **Optimized Docker Images:** Base your inference service container on NVIDIA CUDA images (e.g., `nvcr.io/nvidia/pytorch:23.01-py3` for PyTorch) for optimal GPU utilization.\n",
      "*   **Operational Requirements:**\n",
      "    *   **GPU Driver Management:** The NVIDIA GPU Operator handles this in OKE.\n",
      "    *   **Resource Requests (Kubernetes):** Correctly request GPU resources in your deployment manifests (e.g., `nvidia.com/gpu: 1`).\n",
      "    *   **GPU Monitoring:** Monitor GPU utilization, memory usage, and temperature using custom metrics ingested into OCI Monitoring.\n",
      "*   **Automation Opportunities:**\n",
      "    *   **Automated Docker Image Builds:** OCI DevOps Build Pipelines to build, test, and push GPU-enabled Docker images to OCI Container Registry (OCIR).\n",
      "    *   **Automated GPU Node Provisioning:** Terraform and OKE Cluster Autoscaler handle adding/removing GPU nodes.\n",
      "\n",
      "```dockerfile\n",
      "# Dockerfile snippet for a GPU-accelerated ML inference service\n",
      "FROM nvcr.io/nvidia/pytorch:23.01-py3 # Or appropriate CUDA-enabled base image\n",
      "\n",
      "WORKDIR /app\n",
      "\n",
      "COPY requirements.txt .\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "COPY . .\n",
      "\n",
      "EXPOSE 8080\n",
      "\n",
      "CMD [\"python\", \"app.py\"]\n",
      "```\n",
      "\n",
      "```yaml\n",
      "# Kubernetes deployment snippet for GPU-enabled pod\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: ml-inference-service\n",
      "spec:\n",
      "  template:\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: inference-container\n",
      "        image: phx.ocir.io/tenancy-ocid/ml-inference:latest\n",
      "        resources:\n",
      "          limits:\n",
      "            nvidia.com/gpu: 1 # Request 1 GPU per pod\n",
      "            cpu: \"8\"\n",
      "            memory: \"16Gi\"\n",
      "          requests:\n",
      "            nvidia.com/gpu: 1\n",
      "            cpu: \"4\"\n",
      "            memory: \"8Gi\"\n",
      "        ports:\n",
      "        - containerPort: 8080\n",
      "```\n",
      "\n",
      "### 4. Support model updates without downtime\n",
      "\n",
      "*   **OCI Solution:** **OCI DevOps Deploy Pipelines** combined with **OKE's native deployment strategies**.\n",
      "    *   **Rolling Updates (default in OKE):** Gradually replace old pods with new ones. Health checks within Kubernetes ensure new pods are healthy before old ones are terminated.\n",
      "    *   **Blue/Green Deployments:** Deploy the new model/service version (Green) alongside the old (Blue). Once Green is validated, the OCI Load Balancer (or OKE Ingress) is swapped to direct traffic to Green. If issues, traffic is swapped back to Blue. OCI DevOps pipelines can automate this.\n",
      "    *   **Canary Deployments:** Route a small percentage of traffic to the new version, monitor, and gradually increase traffic. This is ideal for ML models where real-world performance is critical.\n",
      "*   **Operational Requirements:**\n",
      "    *   **Robust Health Checks & Readiness Probes:** Define effective `livenessProbe` and `readinessProbe` in Kubernetes to ensure pods are truly ready to serve traffic.\n",
      "    *   **Automated Rollback:** Plan and automate rollback procedures in case of deployment failures or performance degradation.\n",
      "    *   **Monitoring during Deployment:** Closely monitor key metrics (latency, error rates, GPU utilization) during and after deployments.\n",
      "*   **Automation Opportunities:**\n",
      "    *   **OCI DevOps Deployment Pipeline:** Orchestrate the entire deployment process, including:\n",
      "        1.  Pull new Docker image from OCIR.\n",
      "        2.  Update Kubernetes manifests and apply to OKE.\n",
      "        3.  Automate Blue/Green or Canary traffic shifting logic (e.g., using A/B testing features on the Load Balancer or OKE Ingress).\n",
      "        4.  Run automated smoke tests/validation.\n",
      "        5.  Automated rollback on failure.\n",
      "\n",
      "```yaml\n",
      "# OCI DevOps Deployment Pipeline (simplified pseudocode)\n",
      "stages:\n",
      "  - stageType: KUBERNETES_BLUE_GREEN_DEPLOYMENT # Or KUBERNETES_ROLLING_DEPLOYMENT\n",
      "    name: Deploy ML Inference Service\n",
      "    configuration:\n",
      "      dockerImageArtifacts:\n",
      "        - artifactName: DockerImage-MLInference\n",
      "          path: phx.ocir.io/tenancy-ocid/ml-inference:latest\n",
      "      kubernetesManifests:\n",
      "        - path: kubernetes/deployment.yaml\n",
      "        - path: kubernetes/service.yaml\n",
      "      okeCluster: ${OKE_CLUSTER_ID}\n",
      "      # Blue/Green specific settings\n",
      "      blueServiceSelector: \"app=ml-inference,version=blue\"\n",
      "      greenServiceSelector: \"app=ml-inference,version=green\"\n",
      "      ingressConfig:\n",
      "        type: OCI_LOAD_BALANCER\n",
      "        # ... logic to shift traffic ...\n",
      "      postTrafficShiftWaitSeconds: 300 # Wait for validation\n",
      "    onFailure:\n",
      "      # Automatically trigger rollback\n",
      "      stageOverride:\n",
      "        stageType: KUBERNETES_BLUE_GREEN_DEPLOYMENT_ROLLBACK\n",
      "```\n",
      "\n",
      "### 5. Store inference logs for 90 days\n",
      "\n",
      "*   **OCI Solution:**\n",
      "    *   **OCI Logging Service:** Centralized collection of all logs (OKE, application logs via stdout/stderr).\n",
      "    *   **OCI Service Connector Hub:** To move logs from OCI Logging to **OCI Object Storage** for long-term archival.\n",
      "    *   **OCI Logging Analytics (Optional):** For advanced querying, analysis, and dashboarding of logs.\n",
      "*   **Operational Requirements:**\n",
      "    *   **Structured Logging:** Ensure your application logs are in a structured format (e.g., JSON) for easier parsing and querying. Include request IDs, model versions, outcome, and key input features.\n",
      "    *   **Log Retention Policies:** Configure Object Storage bucket lifecycle policies for 90-day retention and eventual deletion or archival to lower-cost tiers if needed beyond 90 days.\n",
      "    *   **Monitoring Log Volume"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      " STAGE 3: Architecture Synthesis\n",
      "================================================================================\n",
      "As a Senior OCI Solutions Architect, I've reviewed your requirements and the detailed analyses from both Security and DevOps teams. The need for real-time ML inference at scale, with strict latency, high availability, robust security, and cost efficiency, dictates a sophisticated yet agile architecture.\n",
      "\n",
      "---\n",
      "\n",
      "## Executive Summary: Real-Time ML Inference on OCI\n",
      "\n",
      "Our recommended architecture leverages Oracle Cloud Infrastructure's native services to deliver a performant, secure, scalable, and cost-effective real-time ML inference platform. The core will be **Oracle Container Engine for Kubernetes (OKE)** running on **GPU-enabled instances**, providing high-throughput and low-latency inference. This will be protected by an **OCI Web Application Firewall (WAF)** and **Load Balancer**, and managed end-to-end through **OCI DevOps** for continuous integration and delivery.\n",
      "\n",
      "All infrastructure components will be defined and managed as **Infrastructure as Code (IaC)** using **Terraform** via **OCI Resource Manager**, ensuring consistency, repeatability, and version control. Security is embedded from the ground up, following **Zero-Trust principles** with granular **OCI IAM**, **Network Security Groups (NSGs)**, **OCI Vault** for encryption keys, and continuous monitoring via **OCI Cloud Guard** and **Security Zones**. Observability will be comprehensively handled by **OCI Monitoring** and **Logging**, with logs archived to **OCI Object Storage**.\n",
      "\n",
      "This approach provides the necessary performance and agility while adhering to stringent security and operational best practices, all within the specified budget.\n",
      "\n",
      "---\n",
      "\n",
      "## Core Principles Guiding the Architecture\n",
      "\n",
      "1.  **Zero-Trust:** Never trust, always verify. Every component, identity, and network flow is authenticated and authorized.\n",
      "2.  **Least Privilege:** Grant minimum necessary permissions for human and non-human identities.\n",
      "3.  **Defense-in-Depth:** Multiple layers of security controls protect against single points of failure.\n",
      "4.  **Infrastructure as Code (IaC):** All infrastructure defined programmatically, enabling versioning, automation, and consistency.\n",
      "5.  **Event-Driven & Automated Operations:** Proactive monitoring, auto-scaling, and automated deployments/rollbacks.\n",
      "6.  **Cloud Native First:** Leverage OCI's managed services where possible to reduce operational overhead.\n",
      "7.  **Cost Optimization:** Utilize autoscaling, appropriate instance shapes, and lifecycle management for storage.\n",
      "\n",
      "---\n",
      "\n",
      "## Recommended OCI Architecture: Real-Time ML Inference\n",
      "\n",
      "### 1. Architecture Diagram (Conceptual)\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    subgraph OCI Region: us-phoenix-1\n",
      "        subgraph Virtual Cloud Network (VCN)\n",
      "            direction LR\n",
      "            subgraph Public Subnet\n",
      "                OCI_WAF[OCI Web Application Firewall] -- HTTP/S --> OCI_LB[OCI Load Balancer]\n",
      "            end\n",
      "\n",
      "            subgraph Private Subnet (OKE Control Plane & Worker Nodes)\n",
      "                OCI_LB -- HTTP/S --> OKE_INGRESS[OKE Ingress Controller]\n",
      "                subgraph OCI Container Engine for Kubernetes (OKE)\n",
      "                    OKE_INGRESS --> GPU_POD_A[GPU Pod (ML Inference)]\n",
      "                    OKE_INGRESS --> GPU_POD_B[GPU Pod (ML Inference)]\n",
      "                    OKE_INGRESS --> GPU_POD_C[GPU Pod (ML Inference)]\n",
      "                    GPU_POD_A -- K8s Network --> GPU_WORKER_NODE_1(GPU Worker Node - VM.GPU.A10.X)\n",
      "                    GPU_POD_B -- K8s Network --> GPU_WORKER_NODE_2(GPU Worker Node - VM.GPU.A10.X)\n",
      "                    GPU_POD_C -- K8s Network --> GPU_WORKER_NODE_N(GPU Worker Node - VM.GPU.A10.X)\n",
      "\n",
      "                    GPU_WORKER_NODE_1 -- Auto Scaling --> OKE_CA[Cluster Autoscaler]\n",
      "                    GPU_POD_A -- Pod Scaling --> K8s_HPA[Horizontal Pod Autoscaler]\n",
      "                end\n",
      "            end\n",
      "\n",
      "            subgraph Management / DevNet (Dedicated for DevOps/Admin)\n",
      "                OCI_RM[OCI Resource Manager (Terraform)] --> OCI_DEV[OCI DevOps (Build/Deploy)]\n",
      "                OCI_RM -- IaC Provisioning --> VCN\n",
      "                OCI_RM --> OKE\n",
      "                OCI_RM --> OCI_LB\n",
      "                OCI_RM --> OCI_WAF\n",
      "                OCI_DEV --> OCIR[OCI Container Registry]\n",
      "                OCI_DEV --> OKE\n",
      "            end\n",
      "\n",
      "            OKE -- Private Access --> SG[Service Gateway]\n",
      "            OCI_Mon[OCI Monitoring] <-- Metrics --- OKE\n",
      "            OCI_Log[OCI Logging] <-- Logs --- OKE\n",
      "            SG -- Private Service Access --> OCI_OS[OCI Object Storage (Models, Logs)]\n",
      "            SG -- Private Service Access --> OCI_Vault[OCI Vault (CMK, Secrets)]\n",
      "        end\n",
      "    end\n",
      "\n",
      "    EXTERNAL_CLIENTs[External Clients] -- HTTP/S (TLS) --> OCI_WAF\n",
      "\n",
      "    subgraph OCI Management Plane\n",
      "        OCI_IAM[OCI IAM]\n",
      "        OCI_SZ[OCI Security Zones]\n",
      "        OCI_CG[OCI Cloud Guard]\n",
      "        OCI_VSS[OCI Vulnerability Scanning]\n",
      "        OCI_Audit[OCI Audit Logs]\n",
      "    end\n",
      "\n",
      "    OCI_IAM --- OKE\n",
      "    OCI_IAM --- OCI_OS\n",
      "    OCI_IAM --- OCI_Vault\n",
      "    OCI_IAM --- OCI_DEV\n",
      "    OCI_IAM --- OCI_RM\n",
      "    OCI_IAM --- OCI_WAF\n",
      "    OCI_IAM --- OCI_LB\n",
      "\n",
      "    OCI_OS -- CMK Encryption --> OCI_Vault\n",
      "    OCI_Log --> OCI_SCH[OCI Service Connector Hub]\n",
      "    OCI_SCH --> OCI_OS\n",
      "\n",
      "    OCI_VSS -- Scan --> OCIR\n",
      "    OCI_VSS -- Scan --> OKE\n",
      "\n",
      "    OCI_CG -- Alerts --> OCI_Mon\n",
      "    OCI_SZ -- Enforce Policy --> OCI_OS\n",
      "    OCI_SZ -- Enforce Policy --> OKE\n",
      "```\n",
      "\n",
      "## 2. Component Breakdown & Rationale\n",
      "\n",
      "### A. Compute & Scalability (Requirements 1, 2, 3, 4)\n",
      "\n",
      "*   **Oracle\n",
      "\n",
      "\n",
      "================================================================================\n",
      "âœ… REVIEW COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the infrastructure review workflow\n",
    "requirements = \"\"\"\n",
    "We need to deploy a real-time ML inference service with the following requirements:\n",
    "\n",
    "1. Handle 1000 requests per second at peak\n",
    "2. Maximum latency of 100ms for 99th percentile\n",
    "3. Use GPU-accelerated inference\n",
    "4. Support model updates without downtime\n",
    "5. Store inference logs for 90 days\n",
    "6. Operate in us-phoenix-1 region\n",
    "7. Budget: ~$10,000/month\n",
    "\"\"\"\n",
    "\n",
    "review_results = infrastructure_review_workflow(requirements)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
