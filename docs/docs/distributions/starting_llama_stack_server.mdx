---
title: Starting a Llama Stack Server
description: Different ways to run Llama Stack servers - as library, container, or Kubernetes deployment
sidebar_label: Starting Llama Stack Server
sidebar_position: 7
---

# Starting a Llama Stack Server

You can run a Llama Stack server in one of the following ways:

## As a Library:

This is the simplest way to get started. Using Llama Stack as a library means you do not need to start a server. This is especially useful when you are not running inference locally and relying on an external inference service (eg. fireworks, together, groq, etc.) See [Using Llama Stack as a Library](importing_as_library)


## Container:

Another simple way to start interacting with Llama Stack is to just spin up a container (via Docker or Podman) which is pre-built with all the providers you need. We provide a number of pre-built images so you can start a Llama Stack server instantly. You can also build your own custom container. Which distribution to choose depends on the hardware you have. See [Selection of a Distribution](./list_of_distributions) for more details.

## Kubernetes:

If you have built a container image and want to deploy it in a Kubernetes cluster instead of starting the Llama Stack server locally. See [Kubernetes Deployment Guide](../deploying/kubernetes_deployment) for more details.


## Production Server Configuration (Unix/Linux/macOS)

On Unix-based systems (Linux, macOS), Llama Stack automatically uses **Gunicorn with Uvicorn workers** for production-grade multi-process performance. This provides:

- **Multi-process concurrency**: Automatically scales to `(2 Ã— CPU cores) + 1` workers
- **Worker recycling**: Prevents memory leaks by restarting workers periodically
- **High throughput**: Tested at 698+ requests/second with sub-millisecond response times
- **Graceful degradation**: Automatically falls back to single-process Uvicorn on Windows

### Configuration

Configure Gunicorn behavior using environment variables:

- `GUNICORN_WORKERS` or `WEB_CONCURRENCY`: Number of worker processes (default: `(2 * CPU cores) + 1`)
- `GUNICORN_WORKER_CONNECTIONS`: Max concurrent connections per worker (default: `1000`)
- `GUNICORN_TIMEOUT`: Worker timeout in seconds (default: `120`)
- `GUNICORN_KEEPALIVE`: Connection keepalive in seconds (default: `5`)
- `GUNICORN_MAX_REQUESTS`: Restart workers after N requests to prevent memory leaks (default: `10000`)
- `GUNICORN_MAX_REQUESTS_JITTER`: Randomize worker restart timing (default: `1000`)
- `GUNICORN_PRELOAD`: Preload app before forking workers for memory efficiency (default: `true`, as set in `run.py` line 264)

**Important**: When using multiple workers without `GUNICORN_PRELOAD=true`, you may encounter database initialization race conditions. To avoid this, set `GUNICORN_PRELOAD=true` and install all dependencies with `uv sync --group unit --group test`.

**Example production configuration:**
```bash
export GUNICORN_WORKERS=8              # 8 worker processes
export GUNICORN_WORKER_CONNECTIONS=1500 # 12,000 total concurrent capacity
export GUNICORN_PRELOAD=true           # Enable for production
llama stack run starter
```

For more details on distribution-specific configuration, see the [Starter Distribution](./self_hosted_distro/starter) or [NVIDIA Distribution](./self_hosted_distro/nvidia) documentation.

## Configure logging

Control log output via environment variables before starting the server.

- `LLAMA_STACK_LOGGING` sets per-component levels, e.g. `LLAMA_STACK_LOGGING=server=debug;core=info`.
- Supported categories: `all`, `core`, `server`, `router`, `inference`, `agents`, `safety`, `eval`, `tools`, `client`.
- Levels: `debug`, `info`, `warning`, `error`, `critical` (default is `info`). Use `all=<level>` to apply globally.
- `LLAMA_STACK_LOG_FILE=/path/to/log` mirrors logs to a file while still printing to stdout.

Export these variables prior to running `llama stack run`, launching a container, or starting the server through any other pathway.

```{toctree}
:maxdepth: 1
:hidden:

importing_as_library
configuration
```
