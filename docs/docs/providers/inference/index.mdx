---
description: "Llama Stack Inference API for generating completions, chat completions, and embeddings.

    This API provides the raw interface to the underlying models. Two kinds of models are supported:
    - LLM models: these models generate \"raw\" and \"chat\" (conversational) completions.
    - Embedding models: these models generate embeddings to be used for semantic search."
sidebar_label: Inference
title: Inference
---

# Inference

## Overview

Llama Stack Inference API for generating completions, chat completions, and embeddings.

    This API provides the raw interface to the underlying models. Two kinds of models are supported:
    - LLM models: these models generate "raw" and "chat" (conversational) completions.
    - Embedding models: these models generate embeddings to be used for semantic search.

This section contains documentation for all available providers for the **inference** API.

## Providers

- [Meta-Reference](./inline_meta-reference)
- [Sentence-Transformers](./inline_sentence-transformers)
- [Remote - Anthropic](./remote_anthropic)
- [Remote - Azure](./remote_azure)
- [Remote - Bedrock](./remote_bedrock)
- [Remote - Cerebras](./remote_cerebras)
- [Remote - Databricks](./remote_databricks)
- [Remote - Fireworks](./remote_fireworks)
- [Remote - Gemini](./remote_gemini)
- [Remote - Groq](./remote_groq)
- [Remote - Hf - Endpoint](./remote_hf_endpoint)
- [Remote - Hf - Serverless](./remote_hf_serverless)
- [Remote - Llama-Openai-Compat](./remote_llama-openai-compat)
- [Remote - Nvidia](./remote_nvidia)
- [Remote - Ollama](./remote_ollama)
- [Remote - Openai](./remote_openai)
- [Remote - Passthrough](./remote_passthrough)
- [Remote - Runpod](./remote_runpod)
- [Remote - Sambanova](./remote_sambanova)
- [Remote - Tgi](./remote_tgi)
- [Remote - Together](./remote_together)
- [Remote - Vertexai](./remote_vertexai)
- [Remote - Vllm](./remote_vllm)
- [Remote - Watsonx](./remote_watsonx)
